# ğŸ” UAV_PATH_PLANNING ä»£ç é€»è¾‘æ·±åº¦å®¡æŸ¥æŠ¥å‘Š

**å®¡æŸ¥æ—¥æœŸ**: 2025-10-29  
**å®¡æŸ¥æ–¹å¼**: Ultra Think Mode - å¤šä¸“å®¶è§†è§’  
**å®¡æŸ¥é‡ç‚¹**: ä»£ç é€»è¾‘æ­£ç¡®æ€§ï¼ˆä¸è€ƒè™‘ç®—æ³•æ”¹è¿›ï¼‰

---

## ğŸ“‹ ç›®å½•

1. [å®¡æŸ¥æ–¹æ³•è®º](#å®¡æŸ¥æ–¹æ³•è®º)
2. [SACç®—æ³•å®ç°å®¡æŸ¥](#sacç®—æ³•å®ç°å®¡æŸ¥)
3. [MARLç®—æ³•å®ç°å®¡æŸ¥](#marlç®—æ³•å®ç°å®¡æŸ¥)
4. [ç½‘ç»œç»“æ„ä¸å‚æ•°æ›´æ–°å®¡æŸ¥](#ç½‘ç»œç»“æ„ä¸å‚æ•°æ›´æ–°å®¡æŸ¥)
5. [Agentæ“ä½œé€»è¾‘å®¡æŸ¥](#agentæ“ä½œé€»è¾‘å®¡æŸ¥)
6. [PERå®ç°ä¸é›†æˆå®¡æŸ¥](#perå®ç°ä¸é›†æˆå®¡æŸ¥)
7. [è®­ç»ƒé€»è¾‘å®¡æŸ¥](#è®­ç»ƒé€»è¾‘å®¡æŸ¥)
8. [æµ‹è¯•é€»è¾‘å®¡æŸ¥](#æµ‹è¯•é€»è¾‘å®¡æŸ¥)
9. [ç¯å¢ƒBugå®¡æŸ¥](#ç¯å¢ƒbugå®¡æŸ¥)
10. [ä¸¥é‡é—®é¢˜æ±‡æ€»](#ä¸¥é‡é—®é¢˜æ±‡æ€»)
11. [é—®é¢˜ä¼˜å…ˆçº§åˆ†ç±»](#é—®é¢˜ä¼˜å…ˆçº§åˆ†ç±»)

---

## ğŸ¯ å®¡æŸ¥æ–¹æ³•è®º

æœ¬æ¬¡å®¡æŸ¥é‡‡ç”¨**å¤šä¸“å®¶è§†è§’åˆ†ææ³•**ï¼Œåˆ†åˆ«æ‰®æ¼”8ä¸ªä¸åŒé¢†åŸŸçš„ä¸“å®¶å¯¹ä»£ç è¿›è¡Œå®¡æŸ¥ï¼š

1. **SACç®—æ³•ä¸“å®¶** - æ£€æŸ¥SACæ ‡å‡†å®ç°çš„æ­£ç¡®æ€§
2. **å¤šæ™ºèƒ½ä½“RLä¸“å®¶** - æ£€æŸ¥MARLæ¶æ„è®¾è®¡
3. **æ·±åº¦å­¦ä¹ ä¸“å®¶** - æ£€æŸ¥ç¥ç»ç½‘ç»œå®ç°
4. **ç³»ç»Ÿå·¥ç¨‹ä¸“å®¶** - æ£€æŸ¥Agentäº¤äº’é€»è¾‘
5. **æ•°æ®ç»“æ„ä¸“å®¶** - æ£€æŸ¥PERå®ç°
6. **æµç¨‹æ§åˆ¶ä¸“å®¶** - æ£€æŸ¥è®­ç»ƒå¾ªç¯
7. **è¯„ä¼°ä¸“å®¶** - æ£€æŸ¥æµ‹è¯•å’ŒæŒ‡æ ‡è®¡ç®—
8. **ä»¿çœŸç¯å¢ƒä¸“å®¶** - æ£€æŸ¥ç¯å¢ƒçŠ¶æ€è½¬ç§»

---

## 1ï¸âƒ£ SACç®—æ³•å®ç°å®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šSACç®—æ³•ä¸“å®¶
**å®¡æŸ¥ä¾æ®**ï¼šHaarnoja et al. "Soft Actor-Critic Algorithms and Applications" (2018)

### âœ… æ­£ç¡®çš„å®ç°

#### 1.1 é‡å‚æ•°åŒ–æŠ€å·§ï¼ˆReparameterization Trickï¼‰

**ä½ç½®**: `algorithm/masac/agent.py:66-97`

```python
def evaluate(self, state):
    mean, std = self.action_net(state)
    normal = torch.distributions.Normal(mean, std)
    
    # âœ… ä½¿ç”¨ rsample() ä¿æŒæ¢¯åº¦
    x_t = normal.rsample()
    action = torch.tanh(x_t)
    action = torch.clamp(action, self.min_action, self.max_action)
    
    # âœ… æ­£ç¡®çš„ log_prob è®¡ç®—
    log_prob = normal.log_prob(x_t)
    log_prob -= torch.log(1 - action.pow(2) + 1e-6)
    log_prob = log_prob.sum(dim=-1, keepdim=True)
    
    return action, log_prob
```

**åˆ†æ**ï¼š
- âœ… ä½¿ç”¨ `rsample()` è€Œé `sample()`ï¼Œä¿ç•™æ¢¯åº¦
- âœ… å¯¹ tanh å˜æ¢å‰çš„ `x_t` è®¡ç®— log_prob
- âœ… åº”ç”¨ tanh ä¿®æ­£å…¬å¼ï¼š`log Ï€(a|s) = log Î¼(u|s) - log(1-tanhÂ²(u))`
- âœ… å¯¹åŠ¨ä½œç»´åº¦æ±‚å’Œ

**ç»“è®º**ï¼šâœ… **å®Œå…¨æ­£ç¡®**ï¼Œç¬¦åˆSACè®ºæ–‡æ ‡å‡†å®ç°

#### 1.2 Double Q-Network

**ä½ç½®**: `algorithm/masac/model.py:99-200`

```python
class CriticNet(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim, use_layer_norm=True):
        # Q1 ç½‘ç»œ
        self.q1_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_out = nn.Linear(hidden_dim, 1)
        
        # Q2 ç½‘ç»œï¼ˆç‹¬ç«‹å‚æ•°ï¼‰
        self.q2_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_out = nn.Linear(hidden_dim, 1)
```

**åˆ†æ**ï¼š
- âœ… ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„ Q ç½‘ç»œ
- âœ… ç›¸åŒçš„è¾“å…¥ `[state, action]`
- âœ… è¾“å‡ºå•ä¸ª Q å€¼

**ç»“è®º**ï¼šâœ… **å®Œå…¨æ­£ç¡®**

#### 1.3 è‡ªåŠ¨ç†µè°ƒèŠ‚ï¼ˆTemperature Tuningï¼‰

**ä½ç½®**: `algorithm/masac/agent.py:116-142`

```python
class Entropy:
    def __init__(self, target_entropy, lr, device='cpu'):
        self.target_entropy = target_entropy
        self.device = torch.device(device)
        
        # âœ… ä½¿ç”¨ log_alpha ç¡®ä¿ alpha > 0
        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
        self.alpha = self.log_alpha.exp()
        self.optimizer = torch.optim.Adam([self.log_alpha], lr=lr)
```

**åˆ†æ**ï¼š
- âœ… ä½¿ç”¨ `log_alpha` ä¿è¯ `alpha > 0`
- âœ… ç‹¬ç«‹çš„ä¼˜åŒ–å™¨
- âœ… å¯å­¦ä¹ å‚æ•° `requires_grad=True`

**ç»“è®º**ï¼šâœ… **å®Œå…¨æ­£ç¡®**

#### 1.4 è½¯æ›´æ–°ï¼ˆSoft Updateï¼‰

**ä½ç½®**: `algorithm/masac/agent.py:163-166`

```python
def soft_update(self):
    for target_param, param in zip(self.target_critic_net.parameters(), 
                                    self.critic_net.parameters()):
        target_param.data.copy_(
            target_param.data * (1.0 - self.tau) + param.data * self.tau
        )
```

**åˆ†æ**ï¼š
- âœ… Polyak å¹³å‡: `Î¸_target = (1-Ï„)Î¸_target + Ï„Î¸_current`
- âœ… åªæ›´æ–° Critic çš„ç›®æ ‡ç½‘ç»œï¼ˆActor æ— ç›®æ ‡ç½‘ç»œï¼‰

**ç»“è®º**ï¼šâœ… **å®Œå…¨æ­£ç¡®**

### âš ï¸ å‘ç°çš„é—®é¢˜

#### é—®é¢˜ 1.1: ç›®æ ‡ Q å€¼è®¡ç®—ä¸­çš„åŠ¨ä½œç»´åº¦ä¸åŒ¹é… âŒ **ä¸¥é‡**

**ä½ç½®**: `algorithm/masac/trainer.py:399-406`

```python
# é—®é¢˜ä»£ç 
new_action, log_prob_ = actors[i].evaluate(
    b_s_[:, self.state_dim * i : self.state_dim * (i + 1)]  # âœ… æ­£ç¡®ï¼šå•ä¸ªagentçš„çŠ¶æ€
)
target_q1, target_q2 = critics[i].get_target_q_value(
    b_s_,       # âœ… æ­£ç¡®ï¼šå…¨å±€çŠ¶æ€ [batch, state_dim * n_agents]
    new_action  # âŒ é”™è¯¯ï¼šåªæœ‰å•ä¸ªagentçš„åŠ¨ä½œ [batch, action_dim]
)
```

**é—®é¢˜åˆ†æ**ï¼š

æ ¹æ®ä»£ç ï¼ŒCritic ç½‘ç»œçš„å®šä¹‰ï¼š
```python
# trainer.py:286-288
critic = Critic(
    state_dim=self.state_dim * self.n_agents,  # å…¨å±€çŠ¶æ€
    action_dim=self.action_dim,                # å•ä¸ªagentçš„åŠ¨ä½œç»´åº¦
    ...
)
```

Critic ç½‘ç»œæœŸæœ›è¾“å…¥ï¼š
```python
# model.py:115
self.q1_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
# æœŸæœ›è¾“å…¥ç»´åº¦: (7*n_agents) + 2 = 7*n_agents + 2
```

ä½†å®é™…è¾“å…¥ï¼š
- `b_s_`: `[batch, 7*n_agents]` âœ… æ­£ç¡®
- `new_action`: `[batch, 2]` âœ… å•ä¸ªagentçš„åŠ¨ä½œ
- æ‹¼æ¥å: `[batch, 7*n_agents + 2]` âœ… ç»´åº¦åŒ¹é…

**ä½†æ˜¯**ï¼Œè¿™é‡Œå­˜åœ¨**æ¦‚å¿µæ€§é”™è¯¯**ï¼š

åœ¨ MASACï¼ˆMulti-Agent SACï¼‰ä¸­ï¼ŒCritic åº”è¯¥ä½¿ç”¨**å…¨å±€åŠ¨ä½œ**ï¼ˆæ‰€æœ‰agentçš„åŠ¨ä½œï¼‰ï¼Œè€Œä¸æ˜¯å•ä¸ªagentçš„åŠ¨ä½œã€‚è¿™æ˜¯ CTDEï¼ˆCentralized Training, Decentralized Executionï¼‰çš„æ ¸å¿ƒæ€æƒ³ã€‚

**æœŸæœ›çš„å®ç°**ï¼š
```python
# åº”è¯¥æ‹¼æ¥æ‰€æœ‰agentçš„åŠ¨ä½œ
full_actions = []
for j in range(self.n_agents):
    if j == i:
        a, _ = actors[j].evaluate(b_s_[:, self.state_dim*j : self.state_dim*(j+1)])
    else:
        a = b_a[:, self.action_dim*j : self.action_dim*(j+1)]  # ä½¿ç”¨batchä¸­çš„åŠ¨ä½œ
    full_actions.append(a)
full_actions = torch.cat(full_actions, dim=1)  # [batch, action_dim * n_agents]

# Critic åº”è¯¥æ¥æ”¶å…¨å±€åŠ¨ä½œ
target_q1, target_q2 = critics[i].get_target_q_value(b_s_, full_actions)
```

**å½“å‰é—®é¢˜**ï¼š
1. âŒ Critic åªæ¥æ”¶å•ä¸ªagentçš„åŠ¨ä½œï¼Œæ— æ³•å­¦ä¹ å¤šæ™ºèƒ½ä½“åè°ƒ
2. âŒ è¿å CTDE åŸåˆ™
3. âŒ ç½‘ç»œå®šä¹‰ä¸­ `action_dim` åº”è¯¥æ˜¯ `action_dim * n_agents`

**å½±å“**ï¼š
- ä¸¥é‡å½±å“å¤šæ™ºèƒ½ä½“åè°ƒå­¦ä¹ 
- Critic æ— æ³•è¯„ä¼°å…¨å±€çŠ¶æ€-åŠ¨ä½œä»·å€¼
- é€€åŒ–ä¸ºå¤šä¸ªç‹¬ç«‹çš„ SAC agent

**ä¼˜å…ˆçº§**: ğŸ”´ **P0 - ä¸¥é‡**

---

## 2ï¸âƒ£ MARLç®—æ³•å®ç°å®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸“å®¶
**å®¡æŸ¥ä¾æ®**ï¼šCTDE (Centralized Training, Decentralized Execution) èŒƒå¼

### CTDE åŸåˆ™æ£€æŸ¥

CTDE çš„æ ¸å¿ƒæ€æƒ³ï¼š
- **è®­ç»ƒæ—¶**ï¼šCritic ä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼ˆå…¨å±€çŠ¶æ€ + å…¨å±€åŠ¨ä½œï¼‰
- **æ‰§è¡Œæ—¶**ï¼šActor åªä½¿ç”¨å±€éƒ¨è§‚æµ‹ï¼ˆå•ä¸ªagentçš„çŠ¶æ€ï¼‰

### âŒ é—®é¢˜ 2.1: Critic æœªä½¿ç”¨å…¨å±€åŠ¨ä½œ

**ä½ç½®**: `algorithm/masac/trainer.py:286-293` å’Œ `trainer.py:409-410`

```python
# Critic åˆå§‹åŒ–
critic = Critic(
    state_dim=self.state_dim * self.n_agents,  # âœ… å…¨å±€çŠ¶æ€
    action_dim=self.action_dim,                 # âŒ åº”è¯¥æ˜¯ action_dim * n_agents
    ...
)

# Critic æ›´æ–°
current_q1, current_q2 = critics[i].get_q_value(
    b_s,  # âœ… å…¨å±€çŠ¶æ€
    b_a[:, self.action_dim * i : self.action_dim * (i + 1)]  # âŒ åªæœ‰å•ä¸ªagentçš„åŠ¨ä½œ
)
```

**æ­£ç¡®çš„å®ç°åº”è¯¥æ˜¯**ï¼š

```python
# 1. Critic åˆå§‹åŒ–ï¼ˆåº”ä½¿ç”¨å…¨å±€åŠ¨ä½œç»´åº¦ï¼‰
critic = Critic(
    state_dim=self.state_dim * self.n_agents,      # å…¨å±€çŠ¶æ€
    action_dim=self.action_dim * self.n_agents,    # âœ… å…¨å±€åŠ¨ä½œ
    ...
)

# 2. Critic æ›´æ–°ï¼ˆåº”ä½¿ç”¨å…¨å±€åŠ¨ä½œï¼‰
current_q1, current_q2 = critics[i].get_q_value(
    b_s,  # å…¨å±€çŠ¶æ€ [batch, state_dim * n_agents]
    b_a   # âœ… å…¨å±€åŠ¨ä½œ [batch, action_dim * n_agents]
)
```

**ç»“è®º**: âŒ **ä¸¥é‡é”™è¯¯** - å½“å‰å®ç°æ˜¯å¤šä¸ªç‹¬ç«‹çš„ SACï¼Œè€Œä¸æ˜¯ MASAC

**ä¼˜å…ˆçº§**: ğŸ”´ **P0 - ä¸¥é‡**

### âœ… æ­£ç¡®çš„éƒ¨åˆ†

#### 2.1 å»ä¸­å¿ƒåŒ–æ‰§è¡Œ

**ä½ç½®**: `algorithm/masac/trainer.py:347-356`

```python
def _collect_experience(self, actors, observation):
    action = np.zeros((self.n_agents, self.action_dim))
    
    # âœ… æ¯ä¸ªagentç‹¬ç«‹é€‰æ‹©åŠ¨ä½œï¼ˆåªç”¨è‡ªå·±çš„è§‚æµ‹ï¼‰
    for i in range(self.n_agents):
        action[i] = actors[i].choose_action(observation[i])
    
    return action
```

**åˆ†æ**: âœ… **æ­£ç¡®** - æ‰§è¡Œæ—¶æ¯ä¸ªagentåªç”¨å±€éƒ¨è§‚æµ‹

### âš ï¸ é—®é¢˜ 2.2: ç¼ºå°‘æ™ºèƒ½ä½“é—´é€šä¿¡æœºåˆ¶

è™½ç„¶ä»£ç ä¸­æ²¡æœ‰å®ç°é€šä¿¡æœºåˆ¶ï¼Œä½†è¿™æ˜¯**å¯é€‰çš„**ï¼Œä¸ç®—é”™è¯¯ã€‚å½“å‰å®ç°å±äºï¼š
- âœ… **Independent Learners** (å¦‚æœä¿®å¤äº†Criticçš„å…¨å±€åŠ¨ä½œé—®é¢˜)
- âŒ **çœŸæ­£çš„ MASAC** (éœ€è¦ Critic ä½¿ç”¨å…¨å±€åŠ¨ä½œ)

---

## 3ï¸âƒ£ ç½‘ç»œç»“æ„ä¸å‚æ•°æ›´æ–°å®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šæ·±åº¦å­¦ä¹ ä¸“å®¶

### 3.1 ç½‘ç»œç»“æ„æ£€æŸ¥

#### ActorNet ç»“æ„

**ä½ç½®**: `algorithm/masac/model.py:15-97`

```python
class ActorNet(nn.Module):
    def __init__(self, state_dim, action_dim, max_action, hidden_dim, use_layer_norm=True):
        # ç¬¬ä¸€å±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        
        # ç¬¬äºŒå±‚
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        
        # è¾“å‡ºå±‚
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        self.log_std_layer = nn.Linear(hidden_dim, action_dim)
```

**åˆ†æ**ï¼š
- âœ… ä¸¤å±‚éšè—å±‚ï¼ˆ256ç»´ï¼‰
- âœ… ä½¿ç”¨ Layer Normalization
- âœ… ä½¿ç”¨ He åˆå§‹åŒ–
- âœ… ç‹¬ç«‹çš„å‡å€¼å’Œæ ‡å‡†å·®è¾“å‡º
- âœ… `log_std` è£å‰ªåˆ° `[-20, 2]` é˜²æ­¢æ•°å€¼ä¸ç¨³å®š

**ç»“è®º**: âœ… **ç»“æ„åˆç†**

#### CriticNet ç»“æ„

**ä½ç½®**: `algorithm/masac/model.py:99-200`

```python
class CriticNet(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim, use_layer_norm=True):
        # Q1 ç½‘ç»œ
        self.q1_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q1_ln1 = nn.LayerNorm(hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_ln2 = nn.LayerNorm(hidden_dim)
        self.q1_out = nn.Linear(hidden_dim, 1)
        
        # Q2 ç½‘ç»œï¼ˆç‹¬ç«‹å‚æ•°ï¼‰
        # ... åŒæ ·çš„ç»“æ„
```

**åˆ†æ**ï¼š
- âœ… Double Q-Network å‡å°‘è¿‡ä¼°è®¡
- âœ… ä½¿ç”¨ Layer Normalization
- âœ… è¾“å…¥: `[state, action]`
- âŒ **ä½†æ˜¯ `action_dim` å‚æ•°ä¸æ­£ç¡®**ï¼ˆåº”è¯¥æ˜¯å…¨å±€åŠ¨ä½œç»´åº¦ï¼‰

**ç»“è®º**: âš ï¸ **ç»“æ„åˆç†ï¼Œä½†å‚æ•°ç»´åº¦é”™è¯¯**

### 3.2 å‚æ•°æ›´æ–°é€»è¾‘æ£€æŸ¥

#### Critic æ›´æ–°

**ä½ç½®**: `algorithm/masac/trainer.py:408-426`

```python
# è®¡ç®—ç›®æ ‡Qå€¼
target_q = b_r[:, i:(i + 1)] + self.gamma * (
    torch.min(target_q1, target_q2) - 
    entropies[i].alpha * log_prob_
)

# å½“å‰Qå€¼
current_q1, current_q2 = critics[i].get_q_value(b_s, ...)

# âœ… ä½¿ç”¨é‡è¦æ€§é‡‡æ ·æƒé‡
weighted_loss_q1 = (weights * (current_q1 - target_q.detach()) ** 2).mean()
weighted_loss_q2 = (weights * (current_q2 - target_q.detach()) ** 2).mean()
critic_loss = weighted_loss_q1 + weighted_loss_q2

# æ›´æ–°
critics[i].optimizer.zero_grad()
critic_loss.backward()
torch.nn.utils.clip_grad_norm_(critics[i].critic_net.parameters(), max_norm=1.0)
critics[i].optimizer.step()
```

**åˆ†æ**ï¼š
- âœ… ä½¿ç”¨ `torch.min(q1, q2)` å‡å°‘è¿‡ä¼°è®¡
- âœ… ç›®æ ‡å€¼ä½¿ç”¨ `.detach()` åœæ­¢æ¢¯åº¦
- âœ… åº”ç”¨ PER çš„é‡è¦æ€§é‡‡æ ·æƒé‡
- âœ… æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

#### Actor æ›´æ–°

**ä½ç½®**: `algorithm/masac/trainer.py:428-436`

```python
# Actor loss
a, log_prob = actors[i].evaluate(
    b_s[:, self.state_dim * i : self.state_dim * (i + 1)]
)
q1, q2 = critics[i].get_q_value(b_s, a)
q = torch.min(q1, q2)
actor_loss = (entropies[i].alpha * log_prob - q).mean()

# æ›´æ–°
actor_loss_value = actors[i].update(actor_loss)
```

**åˆ†æ**ï¼š
- âœ… SAC çš„ Actor loss: `ğ”¼[Î± log Ï€(a|s) - Q(s,a)]`
- âœ… ä½¿ç”¨ `min(Q1, Q2)` å‡å°‘è¿‡ä¼°è®¡
- âŒ **ä½†æ˜¯ Critic è¾“å…¥çš„åŠ¨ä½œä¸æ­£ç¡®**ï¼ˆåº”è¯¥æ˜¯å…¨å±€åŠ¨ä½œï¼‰

**ç»“è®º**: âš ï¸ **å…¬å¼æ­£ç¡®ï¼Œä½†è¾“å…¥é”™è¯¯**

#### Entropy æ›´æ–°

**ä½ç½®**: `algorithm/masac/trainer.py:438-444`

```python
# Entropy loss
alpha_loss = -(entropies[i].log_alpha.exp() * (
    log_prob + entropies[i].target_entropy
).detach()).mean()

# æ›´æ–°
alpha_loss_value = entropies[i].update(alpha_loss)
```

**åˆ†æ**ï¼š
- âœ… SAC çš„ Î± loss: `ğ”¼[-Î±(log Ï€(a|s) + H_target)]`
- âœ… ä½¿ç”¨ `.detach()` åœæ­¢ log_prob çš„æ¢¯åº¦

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 3.3 ç›®æ ‡ç½‘ç»œæ›´æ–°

**ä½ç½®**: `algorithm/masac/trainer.py:447`

```python
# è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
critics[i].soft_update()
```

**åˆ†æ**ï¼š
- âœ… æ¯æ¬¡è®­ç»ƒåéƒ½æ›´æ–°
- âœ… ä½¿ç”¨ Polyak å¹³å‡
- âœ… Actor æ— ç›®æ ‡ç½‘ç»œï¼ˆSAC æ ‡å‡†ï¼‰

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

---

## 4ï¸âƒ£ Agentæ“ä½œé€»è¾‘å®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šç³»ç»Ÿå·¥ç¨‹ä¸“å®¶

### 4.1 åŠ¨ä½œé€‰æ‹©é€»è¾‘

#### è®­ç»ƒæ—¶ï¼ˆéšæœºç­–ç•¥ï¼‰

**ä½ç½®**: `algorithm/masac/agent.py:24-44`

```python
@torch.no_grad()
def choose_action(self, state):
    state_tensor = torch.FloatTensor(state).to(self.device)
    mean, std = self.action_net(state_tensor)
    distribution = torch.distributions.Normal(mean, std)
    action = distribution.sample()  # âœ… éšæœºé‡‡æ ·
    action = torch.clamp(action, self.min_action, self.max_action)
    return action.cpu().numpy()
```

**åˆ†æ**ï¼š
- âœ… ä½¿ç”¨ `@torch.no_grad()` èŠ‚çœå†…å­˜
- âœ… SAC ä½¿ç”¨éšæœºç­–ç•¥ï¼Œé€šè¿‡é‡‡æ ·è¿›è¡Œæ¢ç´¢
- âœ… åŠ¨ä½œè£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
- âœ… CPU â†’ GPU â†’ CPU è½¬æ¢æ­£ç¡®

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

#### æµ‹è¯•æ—¶ï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰

**ä½ç½®**: `algorithm/masac/agent.py:46-64`

```python
@torch.no_grad()
def choose_action_deterministic(self, state):
    state_tensor = torch.FloatTensor(state).to(self.device)
    mean, _ = self.action_net(state_tensor)  # âœ… åªç”¨å‡å€¼
    action = torch.clamp(mean, self.min_action, self.max_action)
    return action.cpu().numpy()
```

**åˆ†æ**ï¼š
- âœ… æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆå‡å€¼ï¼‰
- âœ… å¿½ç•¥æ ‡å‡†å·®
- âœ… ç¬¦åˆæ ‡å‡†æµ‹è¯•åè®®

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 4.2 ç¯å¢ƒäº¤äº’é€»è¾‘

#### åŠ¨ä½œæ‰§è¡Œ

**ä½ç½®**: `algorithm/masac/trainer.py:719-720`

```python
# é‡‡é›†ç»éªŒ
action = self._collect_experience(actors, observation)

# æ‰§è¡ŒåŠ¨ä½œ
observation_, reward, terminated, truncated, info = self.env.step(action)
```

**åˆ†æ**ï¼š
- âœ… åŠ¨ä½œæ ¼å¼: `[n_agents, action_dim]`
- âœ… è¿”å›å€¼ç¬¦åˆ Gymnasium æ ‡å‡†

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 4.3 ç»éªŒå­˜å‚¨é€»è¾‘

**ä½ç½®**: `algorithm/masac/trainer.py:729-731`

```python
# å­˜å‚¨ç»éªŒ
memory.store(
    observation.flatten(),    # çŠ¶æ€: [n_agents * state_dim]
    action.flatten(),         # åŠ¨ä½œ: [n_agents * action_dim]
    reward.flatten(),         # å¥–åŠ±: [n_agents]
    observation_.flatten()    # ä¸‹ä¸€çŠ¶æ€: [n_agents * state_dim]
)
```

**åˆ†æ**ï¼š
- âœ… æ­£ç¡®å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
- âœ… ç»´åº¦åŒ¹é… `transition_dim`

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### âš ï¸ é—®é¢˜ 4.1: é«˜é¢‘ CPU-GPU æ•°æ®ä¼ è¾“

**ä½ç½®**: `algorithm/masac/agent.py:38-44`

```python
def choose_action(self, state):
    # âŒ æ¯æ¬¡è°ƒç”¨éƒ½ä¼ è¾“
    state_tensor = torch.FloatTensor(state).to(self.device)  # CPU â†’ GPU
    ...
    return action.cpu().numpy()  # GPU â†’ CPU
```

**é—®é¢˜åˆ†æ**ï¼š
- æ¯ä¸ªæ—¶é—´æ­¥è°ƒç”¨ `n_agents` æ¬¡
- é¢‘ç¹çš„ CPU â†” GPU ä¼ è¾“æˆä¸ºæ€§èƒ½ç“¶é¢ˆ

**å»ºè®®**ï¼š
```python
# æ‰¹é‡å¤„ç†æ‰€æœ‰agentçš„åŠ¨ä½œ
def choose_actions_batch(self, states):  # states: [n_agents, state_dim]
    states_tensor = torch.FloatTensor(states).to(self.device)
    ...
    return actions.cpu().numpy()  # ä¸€æ¬¡ä¼ è¾“
```

**ä¼˜å…ˆçº§**: ğŸŸ¡ **P1 - æ€§èƒ½é—®é¢˜**

---

## 5ï¸âƒ£ PERå®ç°ä¸é›†æˆå®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šæ•°æ®ç»“æ„ä¸ç®—æ³•ä¸“å®¶

### 5.1 PER æ•°æ®ç»“æ„

**ä½ç½®**: `algorithm/masac/buffer.py:30-76`

```python
class Memory:
    def __init__(self, capacity, transition_dim, alpha=0.6, beta=0.4, 
                 beta_increment=0.001, epsilon=1e-5):
        self.capacity = capacity
        self.alpha = alpha      # ä¼˜å…ˆçº§æŒ‡æ•°
        self.beta = beta        # ISæƒé‡
        self.epsilon = epsilon  # é˜²æ­¢ä¼˜å…ˆçº§ä¸º0
        
        # âœ… ä½¿ç”¨ float32 èŠ‚çœå†…å­˜
        self.buffer = np.zeros((capacity, transition_dim), dtype=np.float32)
        self.priorities = np.zeros(capacity, dtype=np.float32)
        
        self.counter = 0
        self.max_priority = 1.0
```

**åˆ†æ**ï¼š
- âœ… ä¼˜å…ˆçº§æ•°ç»„ç‹¬ç«‹å­˜å‚¨
- âœ… ä½¿ç”¨ float32 èŠ‚çœ 50% å†…å­˜
- âœ… å‚æ•°è®¾ç½®åˆç† (Î±=0.6, Î²=0.4â†’1.0)

**ç»“è®º**: âœ… **ç»“æ„æ­£ç¡®**

### 5.2 ä¼˜å…ˆçº§é‡‡æ ·

**ä½ç½®**: `algorithm/masac/buffer.py:103-150`

```python
def sample(self, batch_size):
    valid_size = min(self.counter, self.capacity)
    valid_priorities = self.priorities[:valid_size]
    
    # âœ… è®¡ç®—é‡‡æ ·æ¦‚ç‡
    sampling_probs = valid_priorities ** self.alpha
    sampling_probs /= sampling_probs.sum()
    
    # âœ… åŸºäºæ¦‚ç‡é‡‡æ ·
    indices = np.random.choice(
        valid_size, 
        size=batch_size, 
        replace=False,
        p=sampling_probs
    )
    
    batch = self.buffer[indices, :]
    
    # âœ… è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
    weights = (valid_size * sampling_probs[indices]) ** (-self.beta)
    weights /= weights.max()
    
    # âœ… beta é€æ¸å¢å¤§
    self.beta = min(1.0, self.beta + self.beta_increment)
    
    return batch, weights, indices
```

**åˆ†æ**ï¼š
- âœ… é‡‡æ ·æ¦‚ç‡: `P(i) = p_i^Î± / Î£ p_k^Î±`
- âœ… ISæƒé‡: `w_i = (N * P(i))^(-Î²) / max_w`
- âœ… Î² ä» 0.4 å¢é•¿åˆ° 1.0
- âœ… æƒé‡å½’ä¸€åŒ–åˆ° [0, 1]

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 5.3 ä¼˜å…ˆçº§æ›´æ–°

**ä½ç½®**: `algorithm/masac/buffer.py:152-171`

```python
def update_priorities(self, indices, priorities):
    # âœ… æ”¯æŒ tensor è½¬æ¢
    if hasattr(priorities, 'cpu'):
        priorities = priorities.cpu().detach().numpy()
    
    # âœ… æ·»åŠ  epsilon é˜²æ­¢ä¸º0
    priorities = np.abs(priorities) + self.epsilon
    self.priorities[indices] = priorities.flatten()
    
    # âœ… æ›´æ–°æœ€å¤§ä¼˜å…ˆçº§
    self.max_priority = max(self.max_priority, priorities.max())
```

**åˆ†æ**ï¼š
- âœ… ä¼˜å…ˆçº§ = `|TD-error| + Îµ`
- âœ… æ–°ç»éªŒä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
- âœ… å…¼å®¹ PyTorch tensor

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 5.4 è®­ç»ƒé›†æˆ

**ä½ç½®**: `algorithm/masac/trainer.py:372-373` å’Œ `trainer.py:449-451`

```python
# é‡‡æ ·æ—¶è·å–æƒé‡å’Œç´¢å¼•
b_M, weights, indices = memory.sample(self.batch_size)
weights = torch.FloatTensor(weights).to(self.device)

# ... è®­ç»ƒ ...

# è®¡ç®—TD-error
td_error = torch.abs(current_q1 - target_q.detach())
td_errors.append(td_error)

# æ›´æ–°ä¼˜å…ˆçº§
mean_td_error = td_errors[0].cpu().detach().numpy()
memory.update_priorities(indices, mean_td_error)
```

**åˆ†æ**ï¼š
- âœ… æƒé‡åº”ç”¨åˆ° Critic loss
- âœ… ä½¿ç”¨ TD-error æ›´æ–°ä¼˜å…ˆçº§
- âœ… åœ¨è®­ç»ƒåæ›´æ–°

**ç»“è®º**: âœ… **é›†æˆæ­£ç¡®**

### âš ï¸ é—®é¢˜ 5.1: åªä½¿ç”¨ç¬¬ä¸€ä¸ªagentçš„TD-error

**ä½ç½®**: `algorithm/masac/trainer.py:449-451`

```python
# âŒ åªä½¿ç”¨ç¬¬ä¸€ä¸ªagentçš„TD-error
mean_td_error = td_errors[0].cpu().detach().numpy()
memory.update_priorities(indices, mean_td_error)
```

**é—®é¢˜åˆ†æ**ï¼š
- å¤šæ™ºèƒ½ä½“ç¯å¢ƒåº”è¯¥ä½¿ç”¨æ‰€æœ‰agentçš„TD-error
- å½“å‰åªç”¨ `td_errors[0]`ï¼ˆç¬¬ä¸€ä¸ªagentï¼‰

**å»ºè®®ä¿®å¤**ï¼š
```python
# âœ… ä½¿ç”¨æ‰€æœ‰agentçš„å¹³å‡TD-error
all_td_errors = torch.stack(td_errors, dim=0)  # [n_agents, batch, 1]
mean_td_error = all_td_errors.mean(dim=0).cpu().detach().numpy()
memory.update_priorities(indices, mean_td_error)
```

**ä¼˜å…ˆçº§**: ğŸŸ¡ **P1 - å½±å“PERæ•ˆæœ**

---

## 6ï¸âƒ£ è®­ç»ƒé€»è¾‘å®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šæµç¨‹æ§åˆ¶ä¸“å®¶

### 6.1 è®­ç»ƒä¸»å¾ªç¯

**ä½ç½®**: `algorithm/masac/trainer.py:695-757`

```python
for episode in range(ep_max):
    # âœ… æ¯ä¸ªepisodeè®¾ç½®ä¸åŒç§å­
    episode_seed = get_episode_seed(self.base_seed, episode, mode='train')
    set_global_seed(episode_seed, self.deterministic)
    
    # âœ… é‡ç½®ç¯å¢ƒ
    observation, reset_info = self.env.reset()
    
    for timestep in range(ep_len):
        # âœ… é€‰æ‹©åŠ¨ä½œ
        action = self._collect_experience(actors, observation)
        
        # âœ… æ‰§è¡ŒåŠ¨ä½œ
        observation_, reward, terminated, truncated, info = self.env.step(action)
        
        # âœ… å­˜å‚¨ç»éªŒ
        memory.store(observation.flatten(), action.flatten(), 
                   reward.flatten(), observation_.flatten())
        
        # âœ… å­¦ä¹ æ›´æ–°
        if memory.is_ready(self.batch_size):
            stats = self._update_agents(actors, critics, entropies, memory)
        
        # âœ… æ›´æ–°çŠ¶æ€
        observation = observation_
        
        # âœ… æ£€æŸ¥ç»ˆæ­¢
        if done:
            break
```

**åˆ†æ**ï¼š
- âœ… æ ‡å‡†çš„ RL è®­ç»ƒå¾ªç¯
- âœ… ç§å­ç®¡ç†ç¡®ä¿å¯å¤ç°æ€§
- âœ… åªè¦æœ‰è¶³å¤Ÿæ ·æœ¬å°±å¼€å§‹è®­ç»ƒï¼ˆä¸éœ€è¦ç­‰ç¼“å†²åŒºæ»¡ï¼‰
- âœ… æ¯æ­¥éƒ½å°è¯•è®­ç»ƒï¼ˆé«˜æ•ˆï¼‰

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 6.2 æ¨¡å‹ä¿å­˜é€»è¾‘

**ä½ç½®**: `algorithm/masac/trainer.py:465-536`

```python
def _save_models(self, actors, critics, entropies, memory, episode):
    if episode % self.save_interval == 0 and episode > 200:
        leader_save_data = {}
        for i in range(self.n_leader):
            leader_save_data[f'leader_{i}'] = {
                # âœ… Actor
                'actor_net': actors[i].action_net.cpu().state_dict(),
                'actor_opt': actors[i].optimizer.state_dict(),
                # âœ… Critic
                'critic_net': critics[i].critic_net.cpu().state_dict(),
                'critic_opt': critics[i].optimizer.state_dict(),
                'target_critic_net': critics[i].target_critic_net.cpu().state_dict(),
                # âœ… Entropy
                'log_alpha': entropies[i].log_alpha.cpu().detach(),
                'alpha_opt': entropies[i].optimizer.state_dict(),
            }
        # âœ… ä¿å­˜episodeå’Œmemoryç»Ÿè®¡
        leader_save_data['episode'] = episode
        leader_save_data['memory_stats'] = memory.get_stats()
```

**åˆ†æ**ï¼š
- âœ… ä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€
- âœ… åŒ…å« Actor, Critic, ç›®æ ‡ç½‘ç»œ, Entropy
- âœ… åŒ…å«æ‰€æœ‰ä¼˜åŒ–å™¨çŠ¶æ€
- âœ… ä¿å­˜åç§»å› GPU

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 6.3 æ—¥å¿—ç³»ç»Ÿ

**ä½ç½®**: `algorithm/masac/trainer.py:20-45`

```python
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'a', buffering=1)  # âœ… è¡Œç¼“å†²
        self.ansi_escape = re.compile(r'\x1b\[[0-9;]*m')
        
    def write(self, message):
        self.terminal.write(message)  # âœ… ç»ˆç«¯ä¿ç•™é¢œè‰²
        clean_message = self.ansi_escape.sub('', message)  # âœ… æ–‡ä»¶å»é™¤é¢œè‰²
        self.log.write(clean_message)
        self.log.flush()  # âœ… å®æ—¶åˆ·æ–°
```

**åˆ†æ**ï¼š
- âœ… åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
- âœ… å®æ—¶å†™å…¥ï¼Œæ— ç¼“å†²
- âœ… ç»ˆç«¯å‹å¥½ï¼ˆä¿ç•™é¢œè‰²ï¼‰
- âœ… æ–‡ä»¶å‹å¥½ï¼ˆå»é™¤ ANSI ä»£ç ï¼‰

**ç»“è®º**: âœ… **è®¾è®¡ä¼˜ç§€**

---

## 7ï¸âƒ£ æµ‹è¯•é€»è¾‘å®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šè¯„ä¼°ä¸åº¦é‡ä¸“å®¶

### 7.1 æ¨¡å‹åŠ è½½

**ä½ç½®**: `algorithm/masac/tester.py:95-151`

```python
def _load_actors(self):
    actors = []
    
    # åŠ è½½Leaderæ¨¡å‹
    leader_checkpoint = torch.load(self.leader_model_path, map_location=self.device)
    
    for i in range(self.n_leader):
        actor = Actor(...)
        checkpoint_data = leader_checkpoint[f'leader_{i}']
        # âœ… å…¼å®¹æ–°æ—§æ ¼å¼
        if 'actor_net' in checkpoint_data:
            actor.action_net.load_state_dict(checkpoint_data['actor_net'])
        else:
            actor.action_net.load_state_dict(checkpoint_data['net'])
        actors.append(actor)
    
    # FolloweråŒç†...
    return actors
```

**åˆ†æ**ï¼š
- âœ… æ­£ç¡®åŠ è½½æ¯ä¸ªagentçš„ç‹¬ç«‹æƒé‡
- âœ… å…¼å®¹æ–°æ—§ä¿å­˜æ ¼å¼
- âœ… ä½¿ç”¨ `map_location` å¤„ç†è®¾å¤‡

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 7.2 åŠ¨ä½œé€‰æ‹©

**ä½ç½®**: `algorithm/masac/tester.py:153-171`

```python
def _select_actions(self, actors, state):
    action = np.zeros((self.n_agents, self.action_dim))
    
    # âœ… æ¯ä¸ªagentä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
    for i in range(self.n_agents):
        action[i] = actors[i].choose_action_deterministic(state[i])
    
    return action
```

**åˆ†æ**ï¼š
- âœ… æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆå‡å€¼ï¼‰
- âœ… æ¯ä¸ªagentç‹¬ç«‹å†³ç­–
- âœ… ç¬¦åˆæ ‡å‡†æµ‹è¯•åè®®

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 7.3 æŒ‡æ ‡è®¡ç®—

**ä½ç½®**: `algorithm/masac/tester.py:249-260`

```python
# âœ… ä¿®å¤ï¼štimestepæ˜¯ç´¢å¼•ï¼Œæ€»æ­¥æ•°æ˜¯timestep+1
total_steps = timestep + 1
FKR = team_counter / total_steps if total_steps > 0 else 0
average_FKR += FKR
average_timestep += total_steps
average_integral_V += integral_V
average_integral_U += integral_U
all_ep_V.append(integral_V)
all_ep_U.append(integral_U)
all_ep_T.append(total_steps)
all_ep_F.append(FKR)
all_win.append(win)
```

**åˆ†æ**ï¼š
- âœ… **å·²ä¿®å¤**: ä½¿ç”¨ `timestep + 1` ä½œä¸ºæ€»æ­¥æ•°
- âœ… FKR (Formation Keeping Rate): `team_counter / total_steps`
- âœ… è®°å½•æ‰€æœ‰åŸå§‹æ•°æ®

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 7.4 ç»Ÿè®¡åˆ†æ

**ä½ç½®**: `algorithm/masac/tester.py:264-311`

```python
# âœ… æˆåŠŸæ¡ˆä¾‹ç»Ÿè®¡
success_indices = [i for i, w in enumerate(all_win) if w]
if len(success_indices) > 0:
    success_stats = {
        'count': len(success_indices),
        'avg_timestep': np.mean([all_ep_T[i] for i in success_indices]),
        'avg_FKR': np.mean([all_ep_F[i] for i in success_indices]),
        ...
    }

# âœ… å¤±è´¥æ¡ˆä¾‹ç»Ÿè®¡
failure_indices = [i for i, w in enumerate(all_win) if not w]
...

# âœ… è¯¦ç»†è¾“å‡º
print(f"  - ä»»åŠ¡å®Œæˆç‡: {win_times / test_episode:.2%}")
print(f"  - å¹³å‡ç¼–é˜Ÿä¿æŒç‡: {average_FKR / test_episode:.4f} Â± {np.std(all_ep_F):.4f}")
print(f"  - å¹³å‡é£è¡Œæ—¶é—´: {average_timestep / test_episode:.2f} Â± {np.std(all_ep_T):.2f}")
```

**åˆ†æ**ï¼š
- âœ… æ€»ä½“ç»Ÿè®¡ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰
- âœ… æˆåŠŸ/å¤±è´¥æ¡ˆä¾‹åˆ†æ
- âœ… æ‰€æœ‰å…³é”®æŒ‡æ ‡éƒ½è®¡ç®—äº†

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®ï¼Œè®¾è®¡ä¼˜ç§€**

---

## 8ï¸âƒ£ ç¯å¢ƒBugå®¡æŸ¥

### ğŸ“ ä¸“å®¶èº«ä»½ï¼šä»¿çœŸç¯å¢ƒä¸“å®¶

### 8.1 è§‚æµ‹ç©ºé—´å®šä¹‰

**ä½ç½®**: `rl_env/path_env.py:82-89`

```python
# âœ… å®šä¹‰è§‚æµ‹ç©ºé—´ï¼ˆç¬¦åˆ Gymnasium æ ‡å‡†ï¼‰
n_agents = self.leader_num + self.follower_num
obs_low = np.array([[0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0]] * n_agents, dtype=np.float32)
obs_high = np.array([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]] * n_agents, dtype=np.float32)
self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)
```

**åˆ†æ**ï¼š
- âœ… å·²å®šä¹‰ `observation_space`ï¼ˆç¬¦åˆ Gymnasium æ ‡å‡†ï¼‰
- âœ… ç»´åº¦: `[n_agents, 7]`
- âœ… å½’ä¸€åŒ–èŒƒå›´åˆç†

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 8.2 çŠ¶æ€å½’ä¸€åŒ–

**ä½ç½®**: `rl_env/path_env.py:180-227`

```python
# âœ… ä½¿ç”¨å¸¸é‡å®šä¹‰å½’ä¸€åŒ–å‚æ•°
STATE_NORM = {
    'position': 1000.0,
    'speed': 30.0,
    'angle': 360.0,
    'rad_to_deg': 57.3
}

# âœ… å½’ä¸€åŒ–è¾…åŠ©å‡½æ•°
def _normalize_position(self, pos):
    return pos / STATE_NORM['position']

def _normalize_speed(self, speed):
    return speed / STATE_NORM['speed']

def _normalize_angle(self, theta_rad):
    return (theta_rad * STATE_NORM['rad_to_deg']) / STATE_NORM['angle']
```

**åˆ†æ**ï¼š
- âœ… å½’ä¸€åŒ–å‚æ•°é›†ä¸­å®šä¹‰
- âœ… ä½¿ç”¨è¾…åŠ©å‡½æ•°é¿å…é‡å¤ä»£ç 
- âœ… `reset()` å’Œ `step()` ä½¿ç”¨ä¸€è‡´çš„å½’ä¸€åŒ–

**ç»“è®º**: âœ… **è®¾è®¡ä¼˜ç§€ï¼Œå·²ä¿®å¤ä¹‹å‰çš„ä¸ä¸€è‡´é—®é¢˜**

### 8.3 å¥–åŠ±å‡½æ•°

**ä½ç½®**: `rl_env/path_env.py:36-48`

```python
# âœ… å¥–åŠ±å‚æ•°é›†ä¸­å®šä¹‰
REWARD_PARAMS = {
    'collision_penalty': -500.0,
    'warning_penalty': -2.0,
    'boundary_penalty': -1.0,
    'goal_reward': 1000.0,
    'goal_distance_coef': -0.001,
    'formation_distance_coef': -0.001,
    'speed_match_reward': 1.0
}
```

**åˆ†æ**ï¼š
- âœ… å¥–åŠ±å‚æ•°é›†ä¸­å®šä¹‰
- âœ… é¿å…é­”æ³•æ•°å­—
- âš ï¸ å¥–åŠ±å°ºåº¦å·®å¼‚å¤§ (-500 åˆ° +1000)

**å¥–åŠ±å°ºåº¦åˆ†æ**ï¼š
- ç¢°æ’: -500
- åˆ°è¾¾ç›®æ ‡: +1000
- ç¼–é˜Ÿ/ç›®æ ‡è·ç¦»: -0.001 * è·ç¦» â‰ˆ -0.1 åˆ° -1
- é€Ÿåº¦åŒ¹é…: +1

**ç»“è®º**: âš ï¸ **å¥–åŠ±å°ºåº¦ä¸å‡è¡¡ï¼Œä½†è¿™æ˜¯è®¾è®¡é€‰æ‹©ï¼Œä¸ç®—bug**

### 8.4 `step()` è¿”å›å€¼

**ä½ç½®**: `rl_env/path_env.py:470-484`

```python
# âœ… ç¬¦åˆ Gymnasium æ ‡å‡†
observation = copy.deepcopy(self.leader_state).astype(np.float32)
reward = r
terminated = copy.deepcopy(self.done)
truncated = False

info = {
    'win': self.leader.win,
    'team_counter': self.team_counter,
    'leader_reward': float(r[0]),
    'follower_rewards': [float(r[self.leader_num + j]) for j in range(self.follower_num)]
}

return observation, reward, terminated, truncated, info
```

**åˆ†æ**ï¼š
- âœ… è¿”å› 5 å…ƒç»„: `(observation, reward, terminated, truncated, info)`
- âœ… ç¬¦åˆ Gymnasium v0.26+ æ ‡å‡†
- âœ… `info` å­—å…¸åŒ…å«é¢å¤–ä¿¡æ¯

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### 8.5 åŠ¨ä½œè¾¹ç•Œæ£€æŸ¥

**ä½ç½®**: `assignment/components/player.py:136-158`

```python
def update(self, action, Render=False):
    a = action[0]
    phi = action[1]
    if not self.dead:
        self.speed = self.speed + 0.3 * a * dt
        self.theta = self.theta + 0.6 * phi * dt
        self.speed = np.clip(self.speed, 10, 20)  # âœ… Leaderé€Ÿåº¦é™åˆ¶
        # âœ… è§’åº¦ç¯ç»•å¤„ç†
        if self.theta > 2 * math.pi:
            self.theta = self.theta - 2 * math.pi
        elif self.theta < 0:
            self.theta = self.theta + 2 * math.pi
        # ä½ç½®æ›´æ–°
        self.posx += self.speed * math.cos(self.theta) * dt
        self.posy -= self.speed * math.sin(self.theta) * dt
    
    # âœ… è¾¹ç•Œé™åˆ¶
    if self.posx <= C.FLIGHT_AREA_X:
        self.posx = C.FLIGHT_AREA_X
    elif self.posx >= (C.FLIGHT_AREA_X + C.FLIGHT_AREA_WIDTH):
        self.posx = C.FLIGHT_AREA_X + C.FLIGHT_AREA_WIDTH
    # yæ–¹å‘åŒç†...
```

**åˆ†æ**ï¼š
- âœ… é€Ÿåº¦é™åˆ¶: Leader [10, 20], Follower [10, 40]
- âœ… è§’åº¦ç¯ç»•å¤„ç†
- âœ… ä½ç½®è¾¹ç•Œé™åˆ¶
- âœ… Leader å’Œ Follower æœ‰ä¸åŒçš„åŠ¨åŠ›å­¦ç³»æ•°

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

### âš ï¸ é—®é¢˜ 8.1: Leader å’Œ Follower åŠ¨åŠ›å­¦ç³»æ•°ä¸åŒ

**ä½ç½®**: `player.py:140-141` vs `player.py:67-68`

```python
# Leader
self.speed = self.speed + 0.3 * a * dt
self.theta = self.theta + 0.6 * phi * dt

# Follower
self.speed = self.speed + 0.6 * a * dt
self.theta = self.theta + 1.2 * phi * dt
```

**åˆ†æ**ï¼š
- Follower çš„åŠ¨åŠ›å­¦å“åº”æ˜¯ Leader çš„ 2 å€
- è¿™ä½¿å¾— Follower æ›´çµæ´»ï¼Œä½†ä¹Ÿæ›´éš¾æ§åˆ¶
- è¿™æ˜¯**è®¾è®¡é€‰æ‹©**ï¼Œä¸æ˜¯bug

**ç»“è®º**: âš ï¸ **è®¾è®¡åˆç†**ï¼ˆFollower éœ€è¦å¿«é€Ÿè·Ÿéšï¼‰

### âœ… é—®é¢˜ 8.2: `reset()` ä¸­ä½¿ç”¨ `init_x/init_y`

**ä½ç½®**: `rl_env/path_env.py:264-272`

```python
# âœ… æ­£ç¡®ï¼šresetæ—¶ä½¿ç”¨åˆå§‹ä½ç½®
state = [
    self._normalize_position(self.leader.init_x),
    self._normalize_position(self.leader.init_y),
    ...
]
```

**åˆ†æ**ï¼š
- âœ… `reset()` ä½¿ç”¨ `init_x/init_y`ï¼ˆåˆå§‹ä½ç½®ï¼‰
- âœ… `step()` ä½¿ç”¨ `posx/posy`ï¼ˆå½“å‰ä½ç½®ï¼‰
- âœ… è¿™æ˜¯æ­£ç¡®çš„

**ç»“è®º**: âœ… **å®Œå…¨æ­£ç¡®**

---

## ğŸš¨ ä¸¥é‡é—®é¢˜æ±‡æ€»

### ğŸ”´ P0çº§åˆ« - å¿…é¡»ä¿®å¤

#### é—®é¢˜ 1: Critic æœªä½¿ç”¨å…¨å±€åŠ¨ä½œï¼ˆMASACå®ç°é”™è¯¯ï¼‰

**é—®é¢˜æè¿°**ï¼š
- Critic åªæ¥æ”¶å•ä¸ªagentçš„åŠ¨ä½œï¼Œè€Œä¸æ˜¯æ‰€æœ‰agentçš„åŠ¨ä½œ
- è¿å CTDE (Centralized Training, Decentralized Execution) åŸåˆ™
- å¯¼è‡´å¤šæ™ºèƒ½ä½“æ— æ³•å­¦ä¹ åè°ƒ

**ä½ç½®**ï¼š
1. `trainer.py:286-288` - Critic åˆå§‹åŒ–æ—¶ `action_dim` åº”è¯¥æ˜¯ `action_dim * n_agents`
2. `trainer.py:409-410` - `critics[i].get_q_value()` åº”è¯¥æ¥æ”¶å…¨å±€åŠ¨ä½œ
3. `trainer.py:399-406` - `critics[i].get_target_q_value()` åº”è¯¥æ¥æ”¶å…¨å±€åŠ¨ä½œ

**å½±å“**ï¼š
- å½“å‰å®ç°é€€åŒ–ä¸ºå¤šä¸ªç‹¬ç«‹çš„SACï¼Œè€Œä¸æ˜¯çœŸæ­£çš„MASAC
- å¤šæ™ºèƒ½ä½“æ— æ³•å­¦ä¹ åè°ƒç­–ç•¥
- Critic æ— æ³•è¯„ä¼°å…¨å±€çŠ¶æ€-åŠ¨ä½œä»·å€¼

**ä¿®å¤å»ºè®®**ï¼š
```python
# 1. Critic åˆå§‹åŒ–
critic = Critic(
    state_dim=self.state_dim * self.n_agents,
    action_dim=self.action_dim * self.n_agents,  # âœ… å…¨å±€åŠ¨ä½œç»´åº¦
    ...
)

# 2. è®­ç»ƒæ—¶æ„å»ºå…¨å±€åŠ¨ä½œ
full_actions = []
for j in range(self.n_agents):
    if j == i:
        a, _ = actors[j].evaluate(b_s[:, self.state_dim*j:self.state_dim*(j+1)])
    else:
        a = b_a[:, self.action_dim*j:self.action_dim*(j+1)]
    full_actions.append(a)
full_actions = torch.cat(full_actions, dim=1)

# 3. Critic ä½¿ç”¨å…¨å±€åŠ¨ä½œ
current_q1, current_q2 = critics[i].get_q_value(b_s, full_actions)
target_q1, target_q2 = critics[i].get_target_q_value(b_s_, full_actions_next)
```

---

## ğŸŸ¡ é—®é¢˜ä¼˜å…ˆçº§åˆ†ç±»

### ğŸ”´ P0 - ä¸¥é‡é”™è¯¯ï¼ˆå¿…é¡»ä¿®å¤ï¼‰

| é—®é¢˜ | æè¿° | ä½ç½® | å½±å“ |
|------|------|------|------|
| Criticæœªä½¿ç”¨å…¨å±€åŠ¨ä½œ | MASACçš„CTDEå®ç°é”™è¯¯ | trainer.py:286-410 | å¤šæ™ºèƒ½ä½“æ— æ³•åè°ƒ |

### ğŸŸ¡ P1 - é‡è¦é—®é¢˜ï¼ˆå»ºè®®ä¿®å¤ï¼‰

| é—®é¢˜ | æè¿° | ä½ç½® | å½±å“ |
|------|------|------|------|
| é«˜é¢‘CPU-GPUä¼ è¾“ | æ¯ä¸ªæ—¶é—´æ­¥å¤šæ¬¡æ•°æ®ä¼ è¾“ | agent.py:38-44 | è®­ç»ƒé€Ÿåº¦æ…¢ |
| PERåªç”¨é¦–agentçš„TD-error | å¤šæ™ºèƒ½ä½“åº”ä½¿ç”¨å¹³å‡TD-error | trainer.py:449-451 | PERæ•ˆæœä¸ä½³ |

### ğŸŸ¢ P2 - ä¼˜åŒ–å»ºè®®ï¼ˆå¯é€‰ï¼‰

| é—®é¢˜ | æè¿° | ä½ç½® | å½±å“ |
|------|------|------|------|
| å¥–åŠ±å°ºåº¦ä¸å‡è¡¡ | -500åˆ°+1000å·®å¼‚å¤§ | path_env.py:36-48 | è®­ç»ƒç¨³å®šæ€§ |
| åŠ¨åŠ›å­¦ç³»æ•°å·®å¼‚ | Followerå“åº”æ˜¯Leaderçš„2å€ | player.py | æ§åˆ¶éš¾åº¦ |

---

## âœ… å®¡æŸ¥ç»“è®º

### æ€»ä½“è¯„ä»·

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **SACç®—æ³•æ­£ç¡®æ€§** | â­â­â­â­â­ | é‡å‚æ•°åŒ–ã€Double Qã€ç†µè°ƒèŠ‚éƒ½æ­£ç¡® |
| **MARLæ¶æ„** | â­â­ | CTDEå®ç°æœ‰ä¸¥é‡é”™è¯¯ |
| **ç½‘ç»œç»“æ„** | â­â­â­â­â­ | Layer Normã€Heåˆå§‹åŒ–éƒ½å¾ˆå¥½ |
| **Agenté€»è¾‘** | â­â­â­â­â­ | è®­ç»ƒ/æµ‹è¯•ç­–ç•¥åŒºåˆ†æ­£ç¡® |
| **PERå®ç°** | â­â­â­â­â­ | é‡‡æ ·ã€æƒé‡ã€æ›´æ–°éƒ½æ­£ç¡® |
| **è®­ç»ƒæµç¨‹** | â­â­â­â­â­ | ç§å­ç®¡ç†ã€æ—¥å¿—ç³»ç»Ÿä¼˜ç§€ |
| **æµ‹è¯•è¯„ä¼°** | â­â­â­â­â­ | æŒ‡æ ‡è®¡ç®—ã€ç»Ÿè®¡åˆ†æå®Œå–„ |
| **ç¯å¢ƒå®ç°** | â­â­â­â­â­ | ç¬¦åˆGymnasiumæ ‡å‡† |

**æ€»åˆ†**: â­â­â­â­ (4/5)

### æ ¸å¿ƒé—®é¢˜

**å”¯ä¸€çš„ä¸¥é‡é—®é¢˜**ï¼š
- âŒ **Critic æœªä½¿ç”¨å…¨å±€åŠ¨ä½œ** - è¿™æ˜¯MASACå®ç°çš„æ ¸å¿ƒé”™è¯¯

**ä¸ºä»€ä¹ˆå…¶ä»–éƒ¨åˆ†éƒ½å¾ˆå¥½ä½†æ€»åˆ†ä¸é«˜**ï¼š
- å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ˜¯"åè°ƒå­¦ä¹ "
- CTDEçš„å®ç°æ˜¯MASACçš„çµé­‚
- è¿™ä¸ªé”™è¯¯ä½¿å¾—ç®—æ³•é€€åŒ–ä¸ºå¤šä¸ªç‹¬ç«‹çš„SAC
- è™½ç„¶æ¯ä¸ªå•ç‹¬çš„SACå®ç°å¾—å¾ˆå¥½ï¼Œä½†ä¸æ˜¯MASAC

### ä¼˜ç§€çš„è®¾è®¡

1. âœ… **SACç®—æ³•å®ç°å®Œç¾** - é‡å‚æ•°åŒ–ã€ç†µè°ƒèŠ‚ã€è½¯æ›´æ–°éƒ½ç¬¦åˆè®ºæ–‡æ ‡å‡†
2. âœ… **PERå®ç°å®Œæ•´** - ä¼˜å…ˆçº§é‡‡æ ·ã€ISæƒé‡ã€Î²å¢é•¿éƒ½æ­£ç¡®
3. âœ… **å·¥ç¨‹è´¨é‡ä¼˜ç§€** - ç§å­ç®¡ç†ã€æ—¥å¿—ç³»ç»Ÿã€é…ç½®ç®¡ç†éƒ½è¾¾åˆ°ç”Ÿäº§çº§åˆ«
4. âœ… **ä»£ç è§„èŒƒ** - ä½¿ç”¨å¸¸é‡ã€è¾…åŠ©å‡½æ•°ã€æ³¨é‡Šæ¸…æ™°
5. âœ… **æµ‹è¯•è¯„ä¼°å®Œå–„** - ç»Ÿè®¡åˆ†æã€æˆåŠŸ/å¤±è´¥æ¡ˆä¾‹åˆ†æ

### ä¿®å¤åçš„é¢„æœŸæ•ˆæœ

ä¿®å¤ Critic çš„å…¨å±€åŠ¨ä½œé—®é¢˜åï¼š
- âœ… å¤šæ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ åè°ƒç­–ç•¥
- âœ… ç¼–é˜Ÿä¿æŒç‡ï¼ˆFKRï¼‰ä¼šæ˜¾è‘—æé«˜
- âœ… ä»»åŠ¡å®Œæˆç‡ä¼šæå‡
- âœ… çœŸæ­£æˆä¸º MASAC ç®—æ³•

---

## ğŸ“ å…·ä½“ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä¿®å¤ MASAC çš„ CTDE å®ç°

#### æ­¥éª¤ 1: ä¿®æ”¹ Critic åˆå§‹åŒ–

**æ–‡ä»¶**: `algorithm/masac/trainer.py`

```python
# ç¬¬ 284-293 è¡Œï¼Œä¿®æ”¹ Critic åˆå§‹åŒ–
for i in range(self.n_agents):
    # ...
    
    # åˆ›å»º Criticï¼ˆä½¿ç”¨å…¨å±€åŠ¨ä½œç»´åº¦ï¼‰
    critic = Critic(
        state_dim=self.state_dim * self.n_agents,      # å…¨å±€çŠ¶æ€
        action_dim=self.action_dim * self.n_agents,    # âœ… ä¿®æ”¹ï¼šå…¨å±€åŠ¨ä½œ
        hidden_dim=self.hidden_dim,
        value_lr=self.value_lr,
        tau=self.tau,
        device=str(self.device)
    )
    critics.append(critic)
```

#### æ­¥éª¤ 2: ä¿®æ”¹ Critic æ›´æ–°é€»è¾‘

**æ–‡ä»¶**: `algorithm/masac/trainer.py`

```python
# ç¬¬ 397-447 è¡Œï¼Œä¿®æ”¹ _update_agents æ–¹æ³•
for i in range(self.n_agents):
    # === è®¡ç®—ç›®æ ‡ Q å€¼ï¼ˆéœ€è¦å…¨å±€åŠ¨ä½œï¼‰ ===
    
    # æ„å»ºä¸‹ä¸€ä¸ªçŠ¶æ€çš„å…¨å±€åŠ¨ä½œ
    next_actions = []
    next_log_probs = []
    for j in range(self.n_agents):
        a_next, log_p_next = actors[j].evaluate(
            b_s_[:, self.state_dim * j : self.state_dim * (j + 1)]
        )
        next_actions.append(a_next)
        next_log_probs.append(log_p_next)
    
    full_next_actions = torch.cat(next_actions, dim=1)  # [batch, action_dim * n_agents]
    
    # ç›®æ ‡ Q å€¼ï¼ˆä½¿ç”¨å…¨å±€åŠ¨ä½œï¼‰
    target_q1, target_q2 = critics[i].get_target_q_value(b_s_, full_next_actions)
    target_q = b_r[:, i:(i + 1)] + self.gamma * (
        torch.min(target_q1, target_q2) - 
        entropies[i].alpha * next_log_probs[i]  # åªç”¨å½“å‰agentçš„log_prob
    )
    
    # === æ›´æ–° Criticï¼ˆéœ€è¦å…¨å±€åŠ¨ä½œï¼‰ ===
    
    # ä½¿ç”¨batchä¸­çš„å…¨å±€åŠ¨ä½œ
    full_actions = b_a  # [batch, action_dim * n_agents]
    
    current_q1, current_q2 = critics[i].get_q_value(b_s, full_actions)
    
    # TD-error
    td_error = torch.abs(current_q1 - target_q.detach())
    td_errors.append(td_error)
    
    # Critic lossï¼ˆä½¿ç”¨ISæƒé‡ï¼‰
    weighted_loss_q1 = (weights * (current_q1 - target_q.detach()) ** 2).mean()
    weighted_loss_q2 = (weights * (current_q2 - target_q.detach()) ** 2).mean()
    critic_loss = weighted_loss_q1 + weighted_loss_q2
    
    critics[i].optimizer.zero_grad()
    critic_loss.backward()
    torch.nn.utils.clip_grad_norm_(critics[i].critic_net.parameters(), max_norm=1.0)
    critics[i].optimizer.step()
    critic_losses.append(critic_loss.item())
    
    # === æ›´æ–° Actorï¼ˆéœ€è¦å…¨å±€åŠ¨ä½œï¼‰ ===
    
    # æ„å»ºå½“å‰çŠ¶æ€çš„å…¨å±€åŠ¨ä½œ
    current_actions = []
    current_log_probs = []
    for j in range(self.n_agents):
        if j == i:
            # å½“å‰agentä½¿ç”¨æ–°é‡‡æ ·çš„åŠ¨ä½œ
            a_curr, log_p_curr = actors[j].evaluate(
                b_s[:, self.state_dim * j : self.state_dim * (j + 1)]
            )
        else:
            # å…¶ä»–agentä½¿ç”¨batchä¸­çš„åŠ¨ä½œ
            a_curr = b_a[:, self.action_dim * j : self.action_dim * (j + 1)]
            log_p_curr = None
        current_actions.append(a_curr)
        if j == i:
            current_log_probs.append(log_p_curr)
    
    full_current_actions = torch.cat(current_actions, dim=1)
    
    # Actor loss
    q1, q2 = critics[i].get_q_value(b_s, full_current_actions)
    q = torch.min(q1, q2)
    actor_loss = (entropies[i].alpha * current_log_probs[0] - q).mean()
    actor_loss_value = actors[i].update(actor_loss)
    actor_losses.append(actor_loss_value)
    
    # === æ›´æ–° Entropyï¼ˆä¸å˜ï¼‰ ===
    # ... ï¼ˆä¿æŒåŸæœ‰ä»£ç ï¼‰
```

#### æ­¥éª¤ 3: ä¿®å¤ PER çš„ TD-error è®¡ç®—

**æ–‡ä»¶**: `algorithm/masac/trainer.py`

```python
# ç¬¬ 449-451 è¡Œï¼Œä¿®æ”¹ä¼˜å…ˆçº§æ›´æ–°
# ä½¿ç”¨æ‰€æœ‰agentçš„å¹³å‡TD-error
all_td_errors = torch.stack(td_errors, dim=0)  # [n_agents, batch, 1]
mean_td_error = all_td_errors.mean(dim=0).cpu().detach().numpy()
memory.update_priorities(indices, mean_td_error)
```

---

## ğŸ“š å®¡æŸ¥æ€»ç»“

æœ¬æ¬¡ä»£ç å®¡æŸ¥å‘ç°ï¼š

**ä¼˜ç‚¹**ï¼š
1. SACç®—æ³•å®ç°å®Œç¾ï¼ˆé‡å‚æ•°åŒ–ã€ç†µè°ƒèŠ‚ã€è½¯æ›´æ–°ï¼‰
2. PERå®ç°å®Œæ•´æ­£ç¡®
3. å·¥ç¨‹è´¨é‡ä¼˜ç§€ï¼ˆç§å­ç®¡ç†ã€æ—¥å¿—ã€é…ç½®ï¼‰
4. æµ‹è¯•è¯„ä¼°å®Œå–„
5. ç¯å¢ƒç¬¦åˆGymnasiumæ ‡å‡†

**ç¼ºç‚¹**ï¼š
1. MASACçš„CTDEå®ç°é”™è¯¯ï¼ˆCriticæœªä½¿ç”¨å…¨å±€åŠ¨ä½œï¼‰

**å»ºè®®**ï¼š
1. ğŸ”´ **å¿…é¡»ä¿®å¤**: Criticçš„å…¨å±€åŠ¨ä½œé—®é¢˜
2. ğŸŸ¡ **å»ºè®®ä¿®å¤**: PERçš„TD-errorè®¡ç®—
3. ğŸŸ¡ **å»ºè®®ä¼˜åŒ–**: æ‰¹é‡åŠ¨ä½œé€‰æ‹©å‡å°‘CPU-GPUä¼ è¾“

ä¿®å¤åï¼Œç®—æ³•å°†æˆä¸ºçœŸæ­£çš„MASACï¼Œå¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›å°†æ˜¾è‘—æå‡ã€‚

---

**å®¡æŸ¥å®Œæˆæ—¶é—´**: 2025-10-29  
**å®¡æŸ¥äºº**: AI Code Reviewer (Ultra Think Mode)  
**ç‰ˆæœ¬**: v1.0

