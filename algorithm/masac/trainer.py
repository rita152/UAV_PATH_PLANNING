"""
MASAC è®­ç»ƒå™¨
è´Ÿè´£ SAC ç®—æ³•çš„è®­ç»ƒæµç¨‹æ§åˆ¶ã€æ¨¡å‹ä¿å­˜å’Œæ•°æ®è®°å½•
"""
import torch
import numpy as np
from matplotlib import pyplot as plt
import pickle as pkl
import os
import sys
import re
import shutil
from datetime import datetime
from utils import set_global_seed, get_episode_seed, print_seed_info, get_project_root, load_config, set_env_vars
from .agent import Actor, Critic, Entropy
from .buffer import Memory
from rl_env.path_env import RlGame


class Logger:
    """
    åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶çš„æ—¥å¿—ç±»
    å®æ—¶å†™å…¥ï¼Œæ— ç¼“å†²
    ç»ˆç«¯ä¿ç•™é¢œè‰²ï¼Œæ–‡ä»¶å»é™¤ANSIé¢œè‰²ä»£ç 
    """
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'a', buffering=1)  # è¡Œç¼“å†²ï¼Œå®æ—¶å†™å…¥
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºå»é™¤ANSIé¢œè‰²ä»£ç 
        self.ansi_escape = re.compile(r'\x1b\[[0-9;]*m')
        
    def write(self, message):
        # ç»ˆç«¯è¾“å‡ºä¿ç•™é¢œè‰²
        self.terminal.write(message)
        # æ–‡ä»¶è¾“å‡ºå»é™¤é¢œè‰²ä»£ç 
        clean_message = self.ansi_escape.sub('', message)
        self.log.write(clean_message)
        self.log.flush()  # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜
        
    def flush(self):
        self.terminal.flush()
        self.log.flush()
        
    def close(self):
        self.log.close()


class Trainer:
    """
    MASAC è®­ç»ƒå™¨
    
    ç®€åŒ–è®¾è®¡ï¼š
    - ç›´æ¥æ¥å—YAMLé…ç½®æ–‡ä»¶è·¯å¾„
    - è‡ªåŠ¨åˆ›å»ºç¯å¢ƒå’ŒåŠ è½½æ‰€æœ‰å‚æ•°
    - æ”¯æŒé€šè¿‡kwargsè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # ä½¿ç”¨é»˜è®¤é…ç½®
        trainer = Trainer(config="configs/masac/default.yaml")
        trainer.train()
        
        # è¦†ç›–éƒ¨åˆ†å‚æ•°
        trainer = Trainer(config="configs/masac/default.yaml", 
                         ep_max=1000, device='cuda:1')
        trainer.train()
    
    Args:
        config: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        **kwargs: å¯é€‰çš„å‚æ•°è¦†ç›–ï¼ˆä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å¯¹åº”å‚æ•°ï¼‰
    """
    def __init__(self, config, **kwargs):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
            **kwargs: å¯é€‰çš„å‚æ•°è¦†ç›–
        """
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config_path = config
        cfg = load_config(config)
        
        # ä½¿ç”¨kwargsè¦†ç›–é…ç½®
        for key, value in kwargs.items():
            if '.' in key:  # æ”¯æŒåµŒå¥—å‚æ•°ï¼Œå¦‚ training.ep_max
                sections = key.split('.')
                target = cfg
                for section in sections[:-1]:
                    target = target[section]
                target[sections[-1]] = value
            else:
                # è‡ªåŠ¨æŸ¥æ‰¾å¹¶æ›´æ–°å‚æ•°
                for section in cfg.values():
                    if isinstance(section, dict) and key in section:
                        section[key] = value
                        break
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        set_env_vars(cfg)
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        env_cfg = cfg['environment']
        train_cfg = cfg['training']
        net_cfg = cfg['network']
        output_cfg = cfg.get('output', {})
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = RlGame(
            n=env_cfg['n_leader'],
            m=env_cfg['n_follower'],
            render=train_cfg.get('render', False)
        ).unwrapped
        
        # ä»ç¯å¢ƒè·å–åŠ¨ä½œç©ºé—´å‚æ•°
        action_dim = self.env.action_space.shape[0]
        max_action = self.env.action_space.high[0]
        min_action = self.env.action_space.low[0]
        
        # æ™ºèƒ½ä½“æ•°é‡ï¼ˆå†³å®šç½‘ç»œç»“æ„ï¼‰
        self.n_leader = env_cfg['n_leader']
        self.n_follower = env_cfg['n_follower']
        self.n_agents = self.n_leader + self.n_follower
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å‚æ•°
        self.state_dim = env_cfg['state_dim']
        self.action_dim = action_dim
        self.max_action = max_action
        self.min_action = min_action
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = net_cfg['hidden_dim']
        self.q_lr = net_cfg['q_lr']
        self.value_lr = net_cfg['value_lr']
        self.policy_lr = net_cfg['policy_lr']
        self.tau = net_cfg['tau']
        
        # è®­ç»ƒç®—æ³•å‚æ•°
        self.gamma = train_cfg['gamma']
        self.batch_size = train_cfg['batch_size']
        self.memory_capacity = train_cfg['memory_capacity']
        
        # ä¼˜å…ˆçº§ç»éªŒå›æ”¾å‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        self.per_alpha = train_cfg.get('per_alpha', 0.6)
        self.per_beta = train_cfg.get('per_beta', 0.4)
        self.per_beta_increment = train_cfg.get('per_beta_increment', 0.001)
        
        # è®­ç»ƒå‚æ•°ï¼ˆä¿å­˜ä»¥ä¾¿train()ä½¿ç”¨ï¼‰
        self.ep_max = train_cfg['ep_max']
        self.ep_len = train_cfg['ep_len']
        self.train_num = train_cfg['train_num']
        self.render = train_cfg.get('render', False)
        
        # è®¾å¤‡é€‰æ‹©
        device = cfg.get('device', 'auto')
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # æ‰“å°è®¾å¤‡ä¿¡æ¯
        self._print_device_info()
        
        # éšæœºç§å­ç®¡ç†
        seed_cfg = cfg.get('seed', {})
        self.base_seed = seed_cfg.get('base_seed', 42)
        self.deterministic = seed_cfg.get('deterministic', False)
        
        # è®¾ç½®åˆå§‹å…¨å±€ç§å­
        set_global_seed(self.base_seed, self.deterministic)
        print_seed_info(self.base_seed, mode='train', deterministic=self.deterministic)
        
        # å®éªŒé…ç½®
        self.experiment_name = train_cfg.get('experiment_name', 'baseline')
        self.save_dir_prefix = train_cfg.get('save_dir_prefix', 'exp')
        
        # è¾“å‡ºé…ç½®
        self.verbose = output_cfg.get('verbose', True)
        self.log_interval = output_cfg.get('log_interval', 1)
        self.save_interval = output_cfg.get('save_interval', 20)
        
        # åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        self.output_dir = self._create_output_dir(self.experiment_name, self.save_dir_prefix)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬
        self._save_config(self.config_path)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger = None
        self.original_stdout = None
        self._setup_logger()
    
    def _create_output_dir(self, experiment_name, save_dir_prefix):
        """
        åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        
        Args:
            experiment_name: å®éªŒåç§°
            save_dir_prefix: ç›®å½•å‰ç¼€
            
        Returns:
            è¾“å‡ºç›®å½•çš„ç»å¯¹è·¯å¾„
        """
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ›å»ºç›®å½•å: exp_baseline_20251028_143022
        dir_name = f"{save_dir_prefix}_{experiment_name}_{timestamp}"
        
        # å®Œæ•´è·¯å¾„ï¼ˆä½¿ç”¨runsä½œä¸ºæ ¹ç›®å½•ï¼‰
        output_dir = os.path.join(get_project_root(), 'runs', dir_name)
        
        # åˆ›å»ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)
        
        return output_dir
    
    def _setup_logger(self):
        """
        è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼Œå°†è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶å’Œç»ˆç«¯
        """
        log_file = os.path.join(self.output_dir, 'training.log')
        self.original_stdout = sys.stdout
        self.logger = Logger(log_file)
        sys.stdout = self.logger
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        print(f"ğŸ’¡ è®­ç»ƒè¾“å‡ºå°†å®æ—¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶")
    
    def _close_logger(self):
        """
        å…³é—­æ—¥å¿—ç³»ç»Ÿï¼Œæ¢å¤æ ‡å‡†è¾“å‡º
        """
        if self.logger is not None:
            sys.stdout = self.original_stdout
            self.logger.close()
            print(f"âœ… æ—¥å¿—å·²ä¿å­˜: {os.path.join(self.output_dir, 'training.log')}")
    
    def _save_config(self, config_path):
        """
        ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬åˆ°è¾“å‡ºç›®å½•
        
        Args:
            config_path: åŸå§‹é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if config_path and os.path.exists(config_path):
            dest_path = os.path.join(self.output_dir, 'config.yaml')
            shutil.copy(config_path, dest_path)
            print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {dest_path}")
    
    def _print_device_info(self):
        """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(self.device)
            gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9
            print(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print(f"ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ")
    
    def _initialize_agents(self):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“ç»„ä»¶ï¼ˆActor, Critic, Entropyï¼‰
        
        Returns:
            actors, critics, entropies: æ™ºèƒ½ä½“ç»„ä»¶åˆ—è¡¨
        """
        actors = []
        critics = []
        entropies = []
        
        for i in range(self.n_agents):
            # åˆ›å»º Actorï¼ˆç§»åˆ°GPUï¼‰
            actor = Actor(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                max_action=self.max_action,
                min_action=self.min_action,
                hidden_dim=self.hidden_dim,
                policy_lr=self.policy_lr,
                device=str(self.device)
            )
            actors.append(actor)
            
            # åˆ›å»º Criticï¼ˆç§»åˆ°GPUï¼‰
            # CTDE: Critic ä½¿ç”¨å…¨å±€çŠ¶æ€å’Œå…¨å±€åŠ¨ä½œè¿›è¡Œé›†ä¸­å¼è®­ç»ƒ
            critic = Critic(
                state_dim=self.state_dim * self.n_agents,      # å…¨å±€çŠ¶æ€
                action_dim=self.action_dim * self.n_agents,    # å…¨å±€åŠ¨ä½œï¼ˆMASACçš„æ ¸å¿ƒï¼‰
                hidden_dim=self.hidden_dim,
                value_lr=self.value_lr,
                tau=self.tau,
                device=str(self.device)
            )
            critics.append(critic)
            
            # åˆ›å»º Entropy è°ƒèŠ‚å™¨ï¼ˆç§»åˆ°GPUï¼‰
            entropy = Entropy(
                target_entropy=-0.1,
                lr=self.q_lr,
                device=str(self.device)
            )
            entropies.append(entropy)
        
        return actors, critics, entropies
    
    def _initialize_memory(self):
        """
        åˆå§‹åŒ–ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
        
        Returns:
            memory: ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
        """
        transition_dim = (2 * self.state_dim * self.n_agents + 
                         self.action_dim * self.n_agents + 
                         1 * self.n_agents)
        
        # åˆ›å»ºä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
        memory = Memory(
            capacity=self.memory_capacity,
            transition_dim=transition_dim,
            alpha=self.per_alpha,
            beta=self.per_beta,
            beta_increment=self.per_beta_increment
        )
        
        print(f"ğŸ“Š ä¼˜å…ˆçº§ç»éªŒå›æ”¾ï¼ˆPERï¼‰é…ç½®:")
        print(f"  - å®¹é‡: {self.memory_capacity}")
        print(f"  - Alpha: {self.per_alpha} (ä¼˜å…ˆçº§æŒ‡æ•°)")
        print(f"  - Beta: {self.per_beta} â†’ 1.0 (é‡è¦æ€§é‡‡æ ·æƒé‡)")
        print(f"  - å†…å­˜ä¼˜åŒ–: float32 (èŠ‚çœ50%å†…å­˜)")
        
        return memory
    
    def _collect_experience(self, actors, observation):
        """
        é‡‡é›†ç»éªŒï¼ˆé€‰æ‹©åŠ¨ä½œï¼‰
        
        SAC ä½¿ç”¨éšæœºç­–ç•¥ï¼Œä¸éœ€è¦é¢å¤–çš„æ¢ç´¢å™ªå£°
        Actor è¾“å‡ºçš„é«˜æ–¯åˆ†å¸ƒé‡‡æ ·æœ¬èº«å°±æä¾›äº†æ¢ç´¢
        
        Args:
            actors: Actoråˆ—è¡¨
            observation: å½“å‰è§‚æµ‹
            
        Returns:
            action: æ‰§è¡Œçš„åŠ¨ä½œ
        """
        action = np.zeros((self.n_agents, self.action_dim))
        
        # é€‰æ‹©åŠ¨ä½œï¼ˆå·²ç»åŒ…å«éšæœºæ¢ç´¢ï¼‰
        for i in range(self.n_agents):
            action[i] = actors[i].choose_action(observation[i])
        
        # SAC ä¸éœ€è¦é¢å¤–å™ªå£°ï¼Œç­–ç•¥æœ¬èº«æ˜¯éšæœºçš„
        action = np.clip(action, -self.max_action, self.max_action)
        
        return action
    
    def _update_agents(self, actors, critics, entropies, memory):
        """
        æ›´æ–°æ™ºèƒ½ä½“ç½‘ç»œå‚æ•°ï¼ˆä½¿ç”¨ä¼˜å…ˆçº§ç»éªŒå›æ”¾ï¼‰
        
        Args:
            actors: Actoråˆ—è¡¨
            critics: Criticåˆ—è¡¨
            entropies: Entropyåˆ—è¡¨
            memory: ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
            
        Returns:
            stats: è®­ç»ƒç»Ÿè®¡å­—å…¸ï¼ˆåŒ…å«å„ç§losså’Œalphaå€¼ï¼‰
        """
        # ä»ç»éªŒæ± é‡‡æ ·ï¼ˆä¼˜å…ˆçº§é‡‡æ · + é‡è¦æ€§é‡‡æ ·æƒé‡ï¼‰
        b_M, weights, indices = memory.sample(self.batch_size)
        weights = torch.FloatTensor(weights).to(self.device)
        
        # è§£ææ‰¹æ¬¡æ•°æ®
        b_s = b_M[:, :self.state_dim * self.n_agents]
        b_a = b_M[:, self.state_dim * self.n_agents : 
                     self.state_dim * self.n_agents + self.action_dim * self.n_agents]
        b_r = b_M[:, -self.state_dim * self.n_agents - 1 * self.n_agents : 
                     -self.state_dim * self.n_agents]
        b_s_ = b_M[:, -self.state_dim * self.n_agents:]
        
        # è½¬æ¢ä¸º Tensor å¹¶ç§»åˆ° GPU
        b_s = torch.FloatTensor(b_s).to(self.device)
        b_a = torch.FloatTensor(b_a).to(self.device)
        b_r = torch.FloatTensor(b_r).to(self.device)
        b_s_ = torch.FloatTensor(b_s_).to(self.device)
        
        # å­˜å‚¨TD-errorå’ŒæŸå¤±å€¼ç”¨äºæ›´æ–°ä¼˜å…ˆçº§å’Œç›‘æ§
        td_errors = []
        actor_losses = []
        critic_losses = []
        alpha_losses = []
        alphas = []
        
        # æ›´æ–°æ¯ä¸ªæ™ºèƒ½ä½“ï¼ˆCTDE: é›†ä¸­å¼è®­ç»ƒï¼Œå»ä¸­å¿ƒåŒ–æ‰§è¡Œï¼‰
        for i in range(self.n_agents):
            # ===== è®¡ç®—ç›®æ ‡ Q å€¼ï¼ˆä½¿ç”¨å…¨å±€åŠ¨ä½œï¼‰ =====
            # æ„å»ºä¸‹ä¸€ä¸ªçŠ¶æ€çš„å…¨å±€åŠ¨ä½œå‘é‡
            next_actions = []
            next_log_probs = []
            for j in range(self.n_agents):
                a_next, log_p_next = actors[j].evaluate(
                    b_s_[:, self.state_dim * j : self.state_dim * (j + 1)]
                )
                next_actions.append(a_next)
                next_log_probs.append(log_p_next)
            
            # æ‹¼æ¥ä¸ºå…¨å±€åŠ¨ä½œ [batch, action_dim * n_agents]
            full_next_actions = torch.cat(next_actions, dim=1)
            
            # ç›®æ ‡ Q å€¼ï¼ˆCritic ä½¿ç”¨å…¨å±€çŠ¶æ€ + å…¨å±€åŠ¨ä½œï¼‰
            target_q1, target_q2 = critics[i].get_target_q_value(b_s_, full_next_actions)
            target_q = b_r[:, i:(i + 1)] + self.gamma * (
                torch.min(target_q1, target_q2) - 
                entropies[i].alpha * next_log_probs[i]  # åªç”¨å½“å‰agentçš„log_prob
            )
            
            # ===== æ›´æ–° Criticï¼ˆä½¿ç”¨å…¨å±€åŠ¨ä½œï¼‰ =====
            # ä½¿ç”¨batchä¸­çš„å…¨å±€åŠ¨ä½œ
            full_actions = b_a  # [batch, action_dim * n_agents]
            
            current_q1, current_q2 = critics[i].get_q_value(b_s, full_actions)
            
            # è®¡ç®—TD-errorï¼ˆç”¨äºæ›´æ–°ä¼˜å…ˆçº§ï¼‰
            td_error = torch.abs(current_q1 - target_q.detach())
            td_errors.append(td_error)
            
            # Critic loss ä½¿ç”¨ISæƒé‡
            weighted_loss_q1 = (weights * (current_q1 - target_q.detach()) ** 2).mean()
            weighted_loss_q2 = (weights * (current_q2 - target_q.detach()) ** 2).mean()
            critic_loss = weighted_loss_q1 + weighted_loss_q2
            
            critics[i].optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(critics[i].critic_net.parameters(), max_norm=1.0)
            critics[i].optimizer.step()
            critic_losses.append(critic_loss.item())
            
            # ===== æ›´æ–° Actorï¼ˆä½¿ç”¨å…¨å±€åŠ¨ä½œï¼‰ =====
            # æ„å»ºå½“å‰çŠ¶æ€çš„å…¨å±€åŠ¨ä½œå‘é‡ï¼ˆå½“å‰agentä½¿ç”¨æ–°é‡‡æ ·çš„åŠ¨ä½œï¼‰
            current_actions = []
            for j in range(self.n_agents):
                if j == i:
                    # å½“å‰agentä½¿ç”¨æ–°é‡‡æ ·çš„åŠ¨ä½œï¼ˆç”¨äºè®¡ç®—æ¢¯åº¦ï¼‰
                    a_curr, log_p_curr = actors[j].evaluate(
                        b_s[:, self.state_dim * j : self.state_dim * (j + 1)]
                    )
                    current_log_prob = log_p_curr
                else:
                    # å…¶ä»–agentä½¿ç”¨batchä¸­çš„åŠ¨ä½œï¼ˆåœæ­¢æ¢¯åº¦ï¼‰
                    a_curr = b_a[:, self.action_dim * j : self.action_dim * (j + 1)].detach()
                current_actions.append(a_curr)
            
            # æ‹¼æ¥ä¸ºå…¨å±€åŠ¨ä½œ
            full_current_actions = torch.cat(current_actions, dim=1)
            
            # Actor lossï¼ˆCritic è¯„ä¼°å…¨å±€åŠ¨ä½œï¼‰
            q1, q2 = critics[i].get_q_value(b_s, full_current_actions)
            q = torch.min(q1, q2)
            actor_loss = (entropies[i].alpha * current_log_prob - q).mean()
            actor_loss_value = actors[i].update(actor_loss)
            actor_losses.append(actor_loss_value)
            
            # æ›´æ–° Entropy
            alpha_loss = -(entropies[i].log_alpha.exp() * (
                current_log_prob + entropies[i].target_entropy
            ).detach()).mean()
            alpha_loss_value = entropies[i].update(alpha_loss)
            alpha_losses.append(alpha_loss_value)
            alphas.append(entropies[i].alpha.item())
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            critics[i].soft_update()
        
        # æ›´æ–°ä¼˜å…ˆçº§ï¼ˆä½¿ç”¨æ‰€æœ‰æ™ºèƒ½ä½“çš„å¹³å‡TD-errorï¼‰
        all_td_errors = torch.stack(td_errors, dim=0)  # [n_agents, batch, 1]
        mean_td_error = all_td_errors.mean(dim=0).cpu().detach().numpy()
        memory.update_priorities(indices, mean_td_error)
        
        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'actor_loss': np.mean(actor_losses),
            'critic_loss': np.mean(critic_losses),
            'alpha_loss': np.mean(alpha_losses),
            'alpha': np.mean(alphas),
            'td_error': np.mean(mean_td_error),
            'beta': memory.beta
        }
        
        return stats
    
    def _save_models(self, actors, critics, entropies, memory, episode):
        """
        ä¿å­˜å®Œæ•´çš„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆåŒ…å«æ‰€æœ‰ç»„ä»¶ï¼‰
        
        ä¿å­˜å†…å®¹ï¼š
        - Actor ç½‘ç»œå’Œä¼˜åŒ–å™¨
        - Critic ç½‘ç»œå’Œä¼˜åŒ–å™¨ï¼ˆåŒ…æ‹¬ç›®æ ‡ç½‘ç»œï¼‰
        - Entropy å‚æ•°å’Œä¼˜åŒ–å™¨
        - ç»éªŒå›æ”¾ç¼“å†²åŒºç»Ÿè®¡
        - Episode ä¿¡æ¯
        
        Args:
            actors: Actoråˆ—è¡¨
            critics: Criticåˆ—è¡¨
            entropies: Entropyåˆ—è¡¨
            memory: ç»éªŒå›æ”¾ç¼“å†²åŒº
            episode: å½“å‰è½®æ•°
        """
        if episode % self.save_interval == 0 and episode > 200:
            # ä¿å­˜ Leader æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼‰
            leader_save_data = {}
            for i in range(self.n_leader):
                leader_save_data[f'leader_{i}'] = {
                    # Actor
                    'actor_net': actors[i].action_net.cpu().state_dict(),
                    'actor_opt': actors[i].optimizer.state_dict(),
                    # Critic
                    'critic_net': critics[i].critic_net.cpu().state_dict(),
                    'critic_opt': critics[i].optimizer.state_dict(),
                    'target_critic_net': critics[i].target_critic_net.cpu().state_dict(),
                    # Entropy
                    'log_alpha': entropies[i].log_alpha.cpu().detach(),
                    'alpha_opt': entropies[i].optimizer.state_dict(),
                }
            leader_save_data['episode'] = episode
            leader_save_data['memory_stats'] = memory.get_stats()
            torch.save(leader_save_data, os.path.join(self.output_dir, 'leader.pth'))
            
            # ä¿å­˜ Follower æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼‰
            if self.n_follower > 0:
                follower_save_data = {}
                for j in range(self.n_follower):
                    follower_idx = self.n_leader + j
                    follower_save_data[f'follower_{j}'] = {
                        # Actor
                        'actor_net': actors[follower_idx].action_net.cpu().state_dict(),
                        'actor_opt': actors[follower_idx].optimizer.state_dict(),
                        # Critic
                        'critic_net': critics[follower_idx].critic_net.cpu().state_dict(),
                        'critic_opt': critics[follower_idx].optimizer.state_dict(),
                        'target_critic_net': critics[follower_idx].target_critic_net.cpu().state_dict(),
                        # Entropy
                        'log_alpha': entropies[follower_idx].log_alpha.cpu().detach(),
                        'alpha_opt': entropies[follower_idx].optimizer.state_dict(),
                    }
                follower_save_data['episode'] = episode
                torch.save(follower_save_data, os.path.join(self.output_dir, 'follower.pth'))
            
            # ä¿å­˜åç§»å›GPU
            for i in range(self.n_leader):
                actors[i].action_net.to(self.device)
                critics[i].critic_net.to(self.device)
                critics[i].target_critic_net.to(self.device)
                entropies[i].log_alpha = entropies[i].log_alpha.to(self.device)
            
            for j in range(self.n_follower):
                idx = self.n_leader + j
                actors[idx].action_net.to(self.device)
                critics[idx].critic_net.to(self.device)
                critics[idx].target_critic_net.to(self.device)
                entropies[idx].log_alpha = entropies[idx].log_alpha.to(self.device)
    
    def _save_training_data(self, all_ep_r, all_ep_r0, all_ep_r1):
        """
        ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆå¥–åŠ±ç»Ÿè®¡ï¼‰
        
        Args:
            all_ep_r: æ€»å¥–åŠ±åˆ—è¡¨
            all_ep_r0: Leaderå¥–åŠ±åˆ—è¡¨
            all_ep_r1: Followerå¥–åŠ±åˆ—è¡¨
            
        Returns:
            data: ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        all_ep_r_mean = np.mean(np.array(all_ep_r), axis=0)
        all_ep_r_std = np.std(np.array(all_ep_r), axis=0)
        all_ep_L_mean = np.mean(np.array(all_ep_r0), axis=0)
        all_ep_L_std = np.std(np.array(all_ep_r0), axis=0)
        all_ep_F_mean = np.mean(np.array(all_ep_r1), axis=0)
        all_ep_F_std = np.std(np.array(all_ep_r1), axis=0)
        
        data = {
            "all_ep_r_mean": all_ep_r_mean,
            "all_ep_r_std": all_ep_r_std,
            "all_ep_L_mean": all_ep_L_mean,
            "all_ep_L_std": all_ep_L_std,
            "all_ep_F_mean": all_ep_F_mean,
            "all_ep_F_std": all_ep_F_std,
        }
        
        # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
        data_path = os.path.join(self.output_dir, 'training_data.pkl')
        with open(data_path, 'wb') as f:
            pkl.dump(data, f, pkl.HIGHEST_PROTOCOL)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {data_path}")
        
        return data
    
    def _plot_results(self, data):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        
        Args:
            data: è®­ç»ƒç»Ÿè®¡æ•°æ®å­—å…¸
        """
        all_ep_r_mean = data["all_ep_r_mean"]
        all_ep_r_std = data["all_ep_r_std"]
        all_ep_L_mean = data["all_ep_L_mean"]
        all_ep_L_std = data["all_ep_L_std"]
        all_ep_F_mean = data["all_ep_F_mean"]
        all_ep_F_std = data["all_ep_F_std"]
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        all_ep_r_max = all_ep_r_mean + all_ep_r_std * 0.95
        all_ep_r_min = all_ep_r_mean - all_ep_r_std * 0.95
        all_ep_L_max = all_ep_L_mean + all_ep_L_std * 0.95
        all_ep_L_min = all_ep_L_mean - all_ep_L_std * 0.95
        all_ep_F_max = all_ep_F_mean + all_ep_F_std * 0.95
        all_ep_F_min = all_ep_F_mean - all_ep_F_std * 0.95
        
        # ä¿å­˜è·¯å¾„ï¼ˆä½¿ç”¨è¾“å‡ºç›®å½•ï¼‰
        plot_dir = os.path.join(self.output_dir, 'plots')
        
        # ç»˜åˆ¶æ€»å¥–åŠ±æ›²çº¿
        plt.figure(1, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_r_mean)), all_ep_r_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_r_mean)), all_ep_r_max, all_ep_r_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Total reward')
        plt.title('Total Reward Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{plot_dir}/total_reward.png', dpi=300)
        print(f"âœ… æ€»å¥–åŠ±æ›²çº¿å·²ä¿å­˜: {plot_dir}/total_reward.png")
        
        # ç»˜åˆ¶ Leader å¥–åŠ±æ›²çº¿
        plt.figure(2, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_L_mean)), all_ep_L_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_L_mean)), all_ep_L_max, all_ep_L_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Leader reward')
        plt.title('Leader Reward Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{plot_dir}/leader_reward.png', dpi=300)
        print(f"âœ… Leaderå¥–åŠ±æ›²çº¿å·²ä¿å­˜: {plot_dir}/leader_reward.png")
        
        # ç»˜åˆ¶ Follower å¥–åŠ±æ›²çº¿
        plt.figure(3, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_F_mean)), all_ep_F_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_F_mean)), all_ep_F_max, all_ep_F_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Follower reward')
        plt.title('Follower Reward Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{plot_dir}/follower_reward.png', dpi=300)
        print(f"âœ… Followerå¥–åŠ±æ›²çº¿å·²ä¿å­˜: {plot_dir}/follower_reward.png")
    
    def train(self, ep_max=None, ep_len=None, train_num=None, render=None):
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Args (å¯é€‰ï¼Œç”¨äºä¸´æ—¶è¦†ç›–é…ç½®):
            ep_max: æœ€å¤§è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            ep_len: æ¯è½®æœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            train_num: è®­ç»ƒæ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            render: æ˜¯å¦æ¸²æŸ“ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            
        Returns:
            data: è®­ç»ƒç»Ÿè®¡æ•°æ®å­—å…¸
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ä½œä¸ºé»˜è®¤å€¼
        ep_max = ep_max if ep_max is not None else self.ep_max
        ep_len = ep_len if ep_len is not None else self.ep_len
        train_num = train_num if train_num is not None else self.train_num
        render = render if render is not None else self.render
        
        print('SACè®­ç»ƒä¸­...')
        
        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
        all_ep_r = [[] for _ in range(train_num)]
        all_ep_r0 = [[] for _ in range(train_num)]
        all_ep_r1 = [[] for _ in range(train_num)]
        
        for k in range(train_num):
            # åˆå§‹åŒ–ç»„ä»¶
            actors, critics, entropies = self._initialize_agents()
            memory = self._initialize_memory()
            
            # æ‰“å°è¡¨å¤´
            print("\n" + "="*80)
            header_parts = ["Episode"]
            # Leaderåˆ—ï¼ˆåªæœ‰1ä¸ªleaderï¼Œç›´æ¥ä½¿ç”¨"Leader"ï¼‰
            header_parts.append("Leader")
            # Followeråˆ—ï¼ˆæ ¹æ®æ•°é‡æ·»åŠ ï¼‰
            for j in range(self.n_follower):
                if self.n_follower == 1:
                    header_parts.append("Follower")
                else:
                    header_parts.append(f"Follower{j}")
            header_parts.append("Steps")
            header_parts.append("Status")
            print(" | ".join([f"{part:^12}" for part in header_parts]))
            print("="*80)
            
            # è®­ç»ƒå¾ªç¯
            for episode in range(ep_max):
                # ä¸ºæ¯ä¸ªepisodeè®¾ç½®ä¸åŒçš„ç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
                episode_seed = get_episode_seed(self.base_seed, episode, mode='train')
                set_global_seed(episode_seed, self.deterministic)
                
                # é‡ç½®ç¯å¢ƒï¼ˆç¬¦åˆ Gymnasium æ ‡å‡†ï¼‰
                observation, reset_info = self.env.reset()
                reward_total = 0
                
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬ç»Ÿè®¡å¥–åŠ±
                reward_leaders = [0.0] * self.n_leader
                reward_followers = [0.0] * self.n_follower
                
                # è®­ç»ƒç»Ÿè®¡
                episode_stats = {
                    'actor_loss': [],
                    'critic_loss': [],
                    'alpha': [],
                    'td_error': [],
                    'beta': []
                }
                
                for timestep in range(ep_len):
                    # é‡‡é›†ç»éªŒï¼ˆSAC éšæœºç­–ç•¥ï¼Œæ— éœ€é¢å¤–å™ªå£°ï¼‰
                    action = self._collect_experience(actors, observation)
                    
                    # ç¯å¢ƒäº¤äº’ï¼ˆç¬¦åˆ Gymnasium æ ‡å‡†ï¼‰
                    observation_, reward, terminated, truncated, info = self.env.step(action)
                    
                    # ä» info ä¸­æå–é¢å¤–ä¿¡æ¯
                    win = info['win']
                    team_counter = info['team_counter']
                    done = terminated or truncated
                    
                    # å­˜å‚¨ç»éªŒ
                    memory.store(observation.flatten(), action.flatten(), 
                               reward.flatten(), observation_.flatten())
                    
                    # å­¦ä¹ æ›´æ–°ï¼ˆåªè¦æœ‰è¶³å¤Ÿæ ·æœ¬å°±å¼€å§‹è®­ç»ƒï¼Œä¸éœ€è¦ç­‰å¾…ç¼“å†²åŒºæ»¡ï¼‰
                    if memory.is_ready(self.batch_size):
                        stats = self._update_agents(actors, critics, entropies, memory)
                        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                        for key in ['actor_loss', 'critic_loss', 'alpha', 'td_error', 'beta']:
                            episode_stats[key].append(stats[key])
                    
                    # æ›´æ–°çŠ¶æ€
                    observation = observation_
                    reward_total += reward.mean()
                    
                    # åˆ†åˆ«ç´¯ç§¯æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±
                    for i in range(self.n_leader):
                        reward_leaders[i] += float(reward[i])
                    for j in range(self.n_follower):
                        reward_followers[j] += float(reward[self.n_leader + j])
                    
                    # æ¸²æŸ“
                    if render:
                        self.env.render()
                    
                    # æ£€æŸ¥ç»ˆæ­¢
                    if done:
                        break
                
                # åˆ¤æ–­ç»ˆæ­¢çŠ¶æ€
                if done:
                    if win:
                        status = "âœ… Success"
                        status_color = "\033[92m"  # ç»¿è‰²
                    else:
                        status = "âŒ Failure"
                        status_color = "\033[91m"  # çº¢è‰²
                else:
                    status = "â±ï¸  Timeout"
                    status_color = "\033[93m"  # é»„è‰²
                reset_color = "\033[0m"
                
                # æ ¼å¼åŒ–è¾“å‡º
                output_parts = [f"{episode:^12d}"]
                
                # Leaderå¥–åŠ±
                for i in range(self.n_leader):
                    output_parts.append(f"{reward_leaders[i]:^12.2f}")
                
                # Followerå¥–åŠ±
                for j in range(self.n_follower):
                    output_parts.append(f"{reward_followers[j]:^12.2f}")
                
                # æ­¥æ•°å’ŒçŠ¶æ€
                output_parts.append(f"{timestep+1:^12d}")
                output_parts.append(f"{status_color}{status:^12}{reset_color}")
                
                print(" | ".join(output_parts))
                
                # è¾“å‡ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯10ä¸ªepisodeè¾“å‡ºä¸€æ¬¡ï¼‰
                if episode % 10 == 0 and len(episode_stats['actor_loss']) > 0:
                    avg_actor_loss = np.mean(episode_stats['actor_loss'])
                    avg_critic_loss = np.mean(episode_stats['critic_loss'])
                    avg_alpha = np.mean(episode_stats['alpha'])
                    avg_td_error = np.mean(episode_stats['td_error'])
                    current_beta = episode_stats['beta'][-1] if episode_stats['beta'] else 0
                    
                    print(f"  ğŸ“Š ç»Ÿè®¡ [Ep {episode}]: "
                          f"Actor Loss={avg_actor_loss:.4f}, "
                          f"Critic Loss={avg_critic_loss:.4f}, "
                          f"Alpha={avg_alpha:.4f}, "
                          f"TD-error={avg_td_error:.4f}, "
                          f"Beta={current_beta:.4f}")
                
                # è®°å½•ç»Ÿè®¡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                all_ep_r[k].append(reward_total)
                all_ep_r0[k].append(reward_leaders[0])
                if self.n_follower > 0:
                    all_ep_r1[k].append(reward_followers[0])
                
                # ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´æ£€æŸ¥ç‚¹ï¼‰
                self._save_models(actors, critics, entropies, memory, episode)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        data = self._save_training_data(all_ep_r, all_ep_r0, all_ep_r1)
        
        # ç»˜åˆ¶ç»“æœ
        self._plot_results(data)
        
        # å…³é—­æ—¥å¿—
        self._close_logger()
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        return data

