"""
MASAC è®­ç»ƒå™¨
è´Ÿè´£ SAC ç®—æ³•çš„è®­ç»ƒæµç¨‹æ§åˆ¶ã€æ¨¡å‹ä¿å­˜å’Œæ•°æ®è®°å½•
"""
import torch
import numpy as np
from matplotlib import pyplot as plt
import pickle as pkl
import os
import sys
import re
import shutil
from datetime import datetime
from utils import set_global_seed, get_episode_seed, print_seed_info, get_project_root, load_config, set_env_vars
from .agent import Actor, Critic, Entropy
from .buffer import Memory
from .noise import Ornstein_Uhlenbeck_Noise
from rl_env.path_env import RlGame


class Logger:
    """
    åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶çš„æ—¥å¿—ç±»
    å®æ—¶å†™å…¥ï¼Œæ— ç¼“å†²
    ç»ˆç«¯ä¿ç•™é¢œè‰²ï¼Œæ–‡ä»¶å»é™¤ANSIé¢œè‰²ä»£ç 
    """
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'a', buffering=1)  # è¡Œç¼“å†²ï¼Œå®æ—¶å†™å…¥
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºå»é™¤ANSIé¢œè‰²ä»£ç 
        self.ansi_escape = re.compile(r'\x1b\[[0-9;]*m')
        
    def write(self, message):
        # ç»ˆç«¯è¾“å‡ºä¿ç•™é¢œè‰²
        self.terminal.write(message)
        # æ–‡ä»¶è¾“å‡ºå»é™¤é¢œè‰²ä»£ç 
        clean_message = self.ansi_escape.sub('', message)
        self.log.write(clean_message)
        self.log.flush()  # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜
        
    def flush(self):
        self.terminal.flush()
        self.log.flush()
        
    def close(self):
        self.log.close()


class Trainer:
    """
    MASAC è®­ç»ƒå™¨
    
    ç®€åŒ–è®¾è®¡ï¼š
    - ç›´æ¥æ¥å—YAMLé…ç½®æ–‡ä»¶è·¯å¾„
    - è‡ªåŠ¨åˆ›å»ºç¯å¢ƒå’ŒåŠ è½½æ‰€æœ‰å‚æ•°
    - æ”¯æŒé€šè¿‡kwargsè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # ä½¿ç”¨é»˜è®¤é…ç½®
        trainer = Trainer(config="configs/masac/default.yaml")
        trainer.train()
        
        # è¦†ç›–éƒ¨åˆ†å‚æ•°
        trainer = Trainer(config="configs/masac/default.yaml", 
                         ep_max=1000, device='cuda:1')
        trainer.train()
    
    Args:
        config: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        **kwargs: å¯é€‰çš„å‚æ•°è¦†ç›–ï¼ˆä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å¯¹åº”å‚æ•°ï¼‰
    """
    def __init__(self, config, **kwargs):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
            **kwargs: å¯é€‰çš„å‚æ•°è¦†ç›–
        """
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config_path = config
        cfg = load_config(config)
        
        # ä½¿ç”¨kwargsè¦†ç›–é…ç½®
        for key, value in kwargs.items():
            if '.' in key:  # æ”¯æŒåµŒå¥—å‚æ•°ï¼Œå¦‚ training.ep_max
                sections = key.split('.')
                target = cfg
                for section in sections[:-1]:
                    target = target[section]
                target[sections[-1]] = value
            else:
                # è‡ªåŠ¨æŸ¥æ‰¾å¹¶æ›´æ–°å‚æ•°
                for section in cfg.values():
                    if isinstance(section, dict) and key in section:
                        section[key] = value
                        break
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        set_env_vars(cfg)
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        env_cfg = cfg['environment']
        train_cfg = cfg['training']
        net_cfg = cfg['network']
        output_cfg = cfg.get('output', {})
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = RlGame(
            n=env_cfg['n_leader'],
            m=env_cfg['n_follower'],
            render=train_cfg.get('render', False)
        ).unwrapped
        
        # ä»ç¯å¢ƒè·å–åŠ¨ä½œç©ºé—´å‚æ•°
        action_dim = self.env.action_space.shape[0]
        max_action = self.env.action_space.high[0]
        min_action = self.env.action_space.low[0]
        
        # æ™ºèƒ½ä½“æ•°é‡ï¼ˆå†³å®šç½‘ç»œç»“æ„ï¼‰
        self.n_leader = env_cfg['n_leader']
        self.n_follower = env_cfg['n_follower']
        self.n_agents = self.n_leader + self.n_follower
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å‚æ•°
        self.state_dim = env_cfg['state_dim']
        self.action_dim = action_dim
        self.max_action = max_action
        self.min_action = min_action
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = net_cfg['hidden_dim']
        self.q_lr = net_cfg['q_lr']
        self.value_lr = net_cfg['value_lr']
        self.policy_lr = net_cfg['policy_lr']
        self.tau = net_cfg['tau']
        
        # è®­ç»ƒç®—æ³•å‚æ•°
        self.gamma = train_cfg['gamma']
        self.batch_size = train_cfg['batch_size']
        self.memory_capacity = train_cfg['memory_capacity']
        
        # è®­ç»ƒå‚æ•°ï¼ˆä¿å­˜ä»¥ä¾¿train()ä½¿ç”¨ï¼‰
        self.ep_max = train_cfg['ep_max']
        self.ep_len = train_cfg['ep_len']
        self.train_num = train_cfg['train_num']
        self.render = train_cfg.get('render', False)
        
        # è®¾å¤‡é€‰æ‹©
        device = cfg.get('device', 'auto')
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # æ‰“å°è®¾å¤‡ä¿¡æ¯
        self._print_device_info()
        
        # éšæœºç§å­ç®¡ç†
        seed_cfg = cfg.get('seed', {})
        self.base_seed = seed_cfg.get('base_seed', 42)
        self.deterministic = seed_cfg.get('deterministic', False)
        
        # è®¾ç½®åˆå§‹å…¨å±€ç§å­
        set_global_seed(self.base_seed, self.deterministic)
        print_seed_info(self.base_seed, mode='train', deterministic=self.deterministic)
        
        # å®éªŒé…ç½®
        self.experiment_name = train_cfg.get('experiment_name', 'baseline')
        self.save_dir_prefix = train_cfg.get('save_dir_prefix', 'exp')
        
        # è¾“å‡ºé…ç½®
        self.verbose = output_cfg.get('verbose', True)
        self.log_interval = output_cfg.get('log_interval', 1)
        self.save_interval = output_cfg.get('save_interval', 20)
        
        # åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        self.output_dir = self._create_output_dir(self.experiment_name, self.save_dir_prefix)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬
        self._save_config(self.config_path)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger = None
        self.original_stdout = None
        self._setup_logger()
    
    def _create_output_dir(self, experiment_name, save_dir_prefix):
        """
        åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        
        Args:
            experiment_name: å®éªŒåç§°
            save_dir_prefix: ç›®å½•å‰ç¼€
            
        Returns:
            è¾“å‡ºç›®å½•çš„ç»å¯¹è·¯å¾„
        """
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ›å»ºç›®å½•å: exp_baseline_20251028_143022
        dir_name = f"{save_dir_prefix}_{experiment_name}_{timestamp}"
        
        # å®Œæ•´è·¯å¾„ï¼ˆä½¿ç”¨runsä½œä¸ºæ ¹ç›®å½•ï¼‰
        output_dir = os.path.join(get_project_root(), 'runs', dir_name)
        
        # åˆ›å»ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)
        
        return output_dir
    
    def _setup_logger(self):
        """
        è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼Œå°†è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶å’Œç»ˆç«¯
        """
        log_file = os.path.join(self.output_dir, 'training.log')
        self.original_stdout = sys.stdout
        self.logger = Logger(log_file)
        sys.stdout = self.logger
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        print(f"ğŸ’¡ è®­ç»ƒè¾“å‡ºå°†å®æ—¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶")
    
    def _close_logger(self):
        """
        å…³é—­æ—¥å¿—ç³»ç»Ÿï¼Œæ¢å¤æ ‡å‡†è¾“å‡º
        """
        if self.logger is not None:
            sys.stdout = self.original_stdout
            self.logger.close()
            print(f"âœ… æ—¥å¿—å·²ä¿å­˜: {os.path.join(self.output_dir, 'training.log')}")
    
    def _save_config(self, config_path):
        """
        ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬åˆ°è¾“å‡ºç›®å½•
        
        Args:
            config_path: åŸå§‹é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if config_path and os.path.exists(config_path):
            dest_path = os.path.join(self.output_dir, 'config.yaml')
            shutil.copy(config_path, dest_path)
            print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {dest_path}")
    
    def _print_device_info(self):
        """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(self.device)
            gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9
            print(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print(f"ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ")
    
    def _initialize_agents(self):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“ç»„ä»¶ï¼ˆActor, Critic, Entropyï¼‰
        
        Returns:
            actors, critics, entropies: æ™ºèƒ½ä½“ç»„ä»¶åˆ—è¡¨
        """
        actors = []
        critics = []
        entropies = []
        
        for i in range(self.n_agents):
            # åˆ›å»º Actorï¼ˆç§»åˆ°GPUï¼‰
            actor = Actor(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                max_action=self.max_action,
                min_action=self.min_action,
                hidden_dim=self.hidden_dim,
                policy_lr=self.policy_lr,
                device=str(self.device)
            )
            actors.append(actor)
            
            # åˆ›å»º Criticï¼ˆç§»åˆ°GPUï¼‰
            critic = Critic(
                state_dim=self.state_dim * self.n_agents,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                value_lr=self.value_lr,
                tau=self.tau,
                device=str(self.device)
            )
            critics.append(critic)
            
            # åˆ›å»º Entropy è°ƒèŠ‚å™¨ï¼ˆç§»åˆ°GPUï¼‰
            entropy = Entropy(
                target_entropy=-0.1,
                lr=self.q_lr,
                device=str(self.device)
            )
            entropies.append(entropy)
        
        return actors, critics, entropies
    
    def _initialize_memory(self):
        """
        åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº
        
        Returns:
            memory: ç»éªŒå›æ”¾ç¼“å†²åŒº
        """
        transition_dim = (2 * self.state_dim * self.n_agents + 
                         self.action_dim * self.n_agents + 
                         1 * self.n_agents)
        return Memory(capacity=self.memory_capacity, transition_dim=transition_dim)
    
    def _initialize_noise(self):
        """
        åˆå§‹åŒ– OU å™ªå£°ç”Ÿæˆå™¨
        
        Returns:
            noise: OUå™ªå£°ç”Ÿæˆå™¨
        """
        return Ornstein_Uhlenbeck_Noise(
            mean=np.zeros((self.n_agents, self.action_dim)),
            sigma=0.1,
            theta=0.1,
            dt=1e-2
        )
    
    def _collect_experience(self, actors, observation, episode, ou_noise):
        """
        é‡‡é›†ç»éªŒï¼ˆé€‰æ‹©åŠ¨ä½œå¹¶æ·»åŠ å™ªå£°ï¼‰
        
        Args:
            actors: Actoråˆ—è¡¨
            observation: å½“å‰è§‚æµ‹
            episode: å½“å‰è½®æ•°
            ou_noise: OUå™ªå£°ç”Ÿæˆå™¨
            
        Returns:
            action: æ‰§è¡Œçš„åŠ¨ä½œ
        """
        action = np.zeros((self.n_agents, self.action_dim))
        
        # é€‰æ‹©åŠ¨ä½œ
        for i in range(self.n_agents):
            action[i] = actors[i].choose_action(observation[i])
        
        # å‰20è½®æ·»åŠ  OU å™ªå£°è¿›è¡Œæ¢ç´¢
        if episode <= 20:
            noise = ou_noise()
        else:
            noise = 0
        
        action = action + noise
        action = np.clip(action, -self.max_action, self.max_action)
        
        return action
    
    def _update_agents(self, actors, critics, entropies, memory):
        """
        æ›´æ–°æ™ºèƒ½ä½“ç½‘ç»œå‚æ•°
        
        Args:
            actors: Actoråˆ—è¡¨
            critics: Criticåˆ—è¡¨
            entropies: Entropyåˆ—è¡¨
            memory: ç»éªŒå›æ”¾ç¼“å†²åŒº
        """
        # ä»ç»éªŒæ± é‡‡æ ·ï¼ˆCPUæ•°æ®ï¼‰
        b_M = memory.sample(self.batch_size)
        b_s = b_M[:, :self.state_dim * self.n_agents]
        b_a = b_M[:, self.state_dim * self.n_agents : 
                     self.state_dim * self.n_agents + self.action_dim * self.n_agents]
        b_r = b_M[:, -self.state_dim * self.n_agents - 1 * self.n_agents : 
                     -self.state_dim * self.n_agents]
        b_s_ = b_M[:, -self.state_dim * self.n_agents:]
        
        # è½¬æ¢ä¸º Tensor å¹¶ç§»åˆ° GPU
        b_s = torch.FloatTensor(b_s).to(self.device)
        b_a = torch.FloatTensor(b_a).to(self.device)
        b_r = torch.FloatTensor(b_r).to(self.device)
        b_s_ = torch.FloatTensor(b_s_).to(self.device)
        
        # æ›´æ–°æ¯ä¸ªæ™ºèƒ½ä½“
        for i in range(self.n_agents):
            # è®¡ç®—ç›®æ ‡ Q å€¼
            new_action, log_prob_ = actors[i].evaluate(
                b_s_[:, self.state_dim * i : self.state_dim * (i + 1)]
            )
            target_q1, target_q2 = critics[i].get_target_q_value(b_s_, new_action)
            target_q = b_r[:, i:(i + 1)] + self.gamma * (
                torch.min(target_q1, target_q2) - 
                entropies[i].alpha * log_prob_.sum(dim=-1, keepdim=True)
            )
            
            # æ›´æ–° Critic
            current_q1, current_q2 = critics[i].get_q_value(
                b_s, b_a[:, self.action_dim * i : self.action_dim * (i + 1)]
            )
            critics[i].update(current_q1, current_q2, target_q.detach())
            
            # æ›´æ–° Actor
            a, log_prob = actors[i].evaluate(
                b_s[:, self.state_dim * i : self.state_dim * (i + 1)]
            )
            q1, q2 = critics[i].get_q_value(b_s, a)
            q = torch.min(q1, q2)
            actor_loss = (entropies[i].alpha * log_prob.sum(dim=-1, keepdim=True) - q).mean()
            actors[i].update(actor_loss)
            
            # æ›´æ–° Entropy
            alpha_loss = -(entropies[i].log_alpha.exp() * (
                log_prob.sum(dim=-1, keepdim=True) + entropies[i].target_entropy
            ).detach()).mean()
            entropies[i].update(alpha_loss)
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            critics[i].soft_update()
    
    def _save_models(self, actors, episode):
        """
        ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆè‡ªåŠ¨å¤„ç†GPU/CPUï¼‰
        åªä¿å­˜ä¸¤ä¸ªæ–‡ä»¶ï¼šleader.pth å’Œ follower.pth
        follower.pth åŒ…å«æ‰€æœ‰Followerçš„ç‹¬ç«‹æƒé‡
        
        Args:
            actors: Actoråˆ—è¡¨
            episode: å½“å‰è½®æ•°
        """
        if episode % self.save_interval == 0 and episode > 200:
            # ä¿å­˜ Leader æ¨¡å‹ï¼ˆæ‰€æœ‰Leaderçš„æƒé‡ï¼‰
            leader_save_data = {}
            for i in range(self.n_leader):
                leader_save_data[f'leader_{i}'] = {
                    'net': actors[i].action_net.cpu().state_dict(),
                    'opt': actors[i].optimizer.state_dict()
                }
            torch.save(leader_save_data, os.path.join(self.output_dir, 'leader.pth'))
            
            # ä¿å­˜ Follower æ¨¡å‹ï¼ˆæ‰€æœ‰Followerçš„æƒé‡æ‰“åŒ…åˆ°ä¸€ä¸ªæ–‡ä»¶ï¼‰
            if self.n_follower > 0:
                follower_save_data = {}
                for j in range(self.n_follower):
                    follower_idx = self.n_leader + j
                    follower_save_data[f'follower_{j}'] = {
                        'net': actors[follower_idx].action_net.cpu().state_dict(),
                        'opt': actors[follower_idx].optimizer.state_dict()
                    }
                torch.save(follower_save_data, os.path.join(self.output_dir, 'follower.pth'))
            
            # ä¿å­˜åç§»å›GPU
            for i in range(self.n_leader):
                actors[i].action_net.to(self.device)
            for j in range(self.n_follower):
                actors[self.n_leader + j].action_net.to(self.device)
    
    def _save_training_data(self, all_ep_r, all_ep_r0, all_ep_r1):
        """
        ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆå¥–åŠ±ç»Ÿè®¡ï¼‰
        
        Args:
            all_ep_r: æ€»å¥–åŠ±åˆ—è¡¨
            all_ep_r0: Leaderå¥–åŠ±åˆ—è¡¨
            all_ep_r1: Followerå¥–åŠ±åˆ—è¡¨
            
        Returns:
            data: ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        all_ep_r_mean = np.mean(np.array(all_ep_r), axis=0)
        all_ep_r_std = np.std(np.array(all_ep_r), axis=0)
        all_ep_L_mean = np.mean(np.array(all_ep_r0), axis=0)
        all_ep_L_std = np.std(np.array(all_ep_r0), axis=0)
        all_ep_F_mean = np.mean(np.array(all_ep_r1), axis=0)
        all_ep_F_std = np.std(np.array(all_ep_r1), axis=0)
        
        data = {
            "all_ep_r_mean": all_ep_r_mean,
            "all_ep_r_std": all_ep_r_std,
            "all_ep_L_mean": all_ep_L_mean,
            "all_ep_L_std": all_ep_L_std,
            "all_ep_F_mean": all_ep_F_mean,
            "all_ep_F_std": all_ep_F_std,
        }
        
        # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
        data_path = os.path.join(self.output_dir, 'training_data.pkl')
        with open(data_path, 'wb') as f:
            pkl.dump(data, f, pkl.HIGHEST_PROTOCOL)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {data_path}")
        
        return data
    
    def _plot_results(self, data):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        
        Args:
            data: è®­ç»ƒç»Ÿè®¡æ•°æ®å­—å…¸
        """
        all_ep_r_mean = data["all_ep_r_mean"]
        all_ep_r_std = data["all_ep_r_std"]
        all_ep_L_mean = data["all_ep_L_mean"]
        all_ep_L_std = data["all_ep_L_std"]
        all_ep_F_mean = data["all_ep_F_mean"]
        all_ep_F_std = data["all_ep_F_std"]
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        all_ep_r_max = all_ep_r_mean + all_ep_r_std * 0.95
        all_ep_r_min = all_ep_r_mean - all_ep_r_std * 0.95
        all_ep_L_max = all_ep_L_mean + all_ep_L_std * 0.95
        all_ep_L_min = all_ep_L_mean - all_ep_L_std * 0.95
        all_ep_F_max = all_ep_F_mean + all_ep_F_std * 0.95
        all_ep_F_min = all_ep_F_mean - all_ep_F_std * 0.95
        
        # ä¿å­˜è·¯å¾„ï¼ˆä½¿ç”¨è¾“å‡ºç›®å½•ï¼‰
        plot_dir = os.path.join(self.output_dir, 'plots')
        
        # ç»˜åˆ¶æ€»å¥–åŠ±æ›²çº¿
        plt.figure(1, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_r_mean)), all_ep_r_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_r_mean)), all_ep_r_max, all_ep_r_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Total reward')
        plt.title('Total Reward Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{plot_dir}/total_reward.png', dpi=300)
        print(f"âœ… æ€»å¥–åŠ±æ›²çº¿å·²ä¿å­˜: {plot_dir}/total_reward.png")
        
        # ç»˜åˆ¶ Leader å¥–åŠ±æ›²çº¿
        plt.figure(2, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_L_mean)), all_ep_L_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_L_mean)), all_ep_L_max, all_ep_L_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Leader reward')
        plt.title('Leader Reward Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{plot_dir}/leader_reward.png', dpi=300)
        print(f"âœ… Leaderå¥–åŠ±æ›²çº¿å·²ä¿å­˜: {plot_dir}/leader_reward.png")
        
        # ç»˜åˆ¶ Follower å¥–åŠ±æ›²çº¿
        plt.figure(3, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_F_mean)), all_ep_F_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_F_mean)), all_ep_F_max, all_ep_F_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Follower reward')
        plt.title('Follower Reward Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{plot_dir}/follower_reward.png', dpi=300)
        print(f"âœ… Followerå¥–åŠ±æ›²çº¿å·²ä¿å­˜: {plot_dir}/follower_reward.png")
    
    def train(self, ep_max=None, ep_len=None, train_num=None, render=None):
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Args (å¯é€‰ï¼Œç”¨äºä¸´æ—¶è¦†ç›–é…ç½®):
            ep_max: æœ€å¤§è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            ep_len: æ¯è½®æœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            train_num: è®­ç»ƒæ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            render: æ˜¯å¦æ¸²æŸ“ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
            
        Returns:
            data: è®­ç»ƒç»Ÿè®¡æ•°æ®å­—å…¸
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ä½œä¸ºé»˜è®¤å€¼
        ep_max = ep_max if ep_max is not None else self.ep_max
        ep_len = ep_len if ep_len is not None else self.ep_len
        train_num = train_num if train_num is not None else self.train_num
        render = render if render is not None else self.render
        
        print('SACè®­ç»ƒä¸­...')
        
        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
        all_ep_r = [[] for _ in range(train_num)]
        all_ep_r0 = [[] for _ in range(train_num)]
        all_ep_r1 = [[] for _ in range(train_num)]
        
        for k in range(train_num):
            # åˆå§‹åŒ–ç»„ä»¶
            actors, critics, entropies = self._initialize_agents()
            memory = self._initialize_memory()
            ou_noise = self._initialize_noise()
            
            # æ‰“å°è¡¨å¤´
            print("\n" + "="*80)
            header_parts = ["Episode"]
            # Leaderåˆ—ï¼ˆåªæœ‰1ä¸ªleaderï¼Œç›´æ¥ä½¿ç”¨"Leader"ï¼‰
            header_parts.append("Leader")
            # Followeråˆ—ï¼ˆæ ¹æ®æ•°é‡æ·»åŠ ï¼‰
            for j in range(self.n_follower):
                if self.n_follower == 1:
                    header_parts.append("Follower")
                else:
                    header_parts.append(f"Follower{j}")
            header_parts.append("Steps")
            header_parts.append("Status")
            print(" | ".join([f"{part:^12}" for part in header_parts]))
            print("="*80)
            
            # è®­ç»ƒå¾ªç¯
            for episode in range(ep_max):
                # ä¸ºæ¯ä¸ªepisodeè®¾ç½®ä¸åŒçš„ç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
                episode_seed = get_episode_seed(self.base_seed, episode, mode='train')
                set_global_seed(episode_seed, self.deterministic)
                
                observation = self.env.reset()
                reward_total = 0
                
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬ç»Ÿè®¡å¥–åŠ±
                reward_leaders = [0.0] * self.n_leader
                reward_followers = [0.0] * self.n_follower
                
                for timestep in range(ep_len):
                    # é‡‡é›†ç»éªŒ
                    action = self._collect_experience(actors, observation, episode, ou_noise)
                    
                    # ç¯å¢ƒäº¤äº’
                    observation_, reward, done, win, team_counter = self.env.step(action)
                    
                    # å­˜å‚¨ç»éªŒ
                    memory.store(observation.flatten(), action.flatten(), 
                               reward.flatten(), observation_.flatten())
                    
                    # å­¦ä¹ æ›´æ–°
                    if memory.counter > self.memory_capacity:
                        self._update_agents(actors, critics, entropies, memory)
                    
                    # æ›´æ–°çŠ¶æ€
                    observation = observation_
                    reward_total += reward.mean()
                    
                    # åˆ†åˆ«ç´¯ç§¯æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±
                    for i in range(self.n_leader):
                        reward_leaders[i] += float(reward[i])
                    for j in range(self.n_follower):
                        reward_followers[j] += float(reward[self.n_leader + j])
                    
                    # æ¸²æŸ“
                    if render:
                        self.env.render()
                    
                    # æ£€æŸ¥ç»ˆæ­¢
                    if done:
                        break
                
                # åˆ¤æ–­ç»ˆæ­¢çŠ¶æ€
                if done:
                    if win:
                        status = "âœ… Success"
                        status_color = "\033[92m"  # ç»¿è‰²
                    else:
                        status = "âŒ Failure"
                        status_color = "\033[91m"  # çº¢è‰²
                else:
                    status = "â±ï¸  Timeout"
                    status_color = "\033[93m"  # é»„è‰²
                reset_color = "\033[0m"
                
                # æ ¼å¼åŒ–è¾“å‡º
                output_parts = [f"{episode:^12d}"]
                
                # Leaderå¥–åŠ±
                for i in range(self.n_leader):
                    output_parts.append(f"{reward_leaders[i]:^12.2f}")
                
                # Followerå¥–åŠ±
                for j in range(self.n_follower):
                    output_parts.append(f"{reward_followers[j]:^12.2f}")
                
                # æ­¥æ•°å’ŒçŠ¶æ€
                output_parts.append(f"{timestep+1:^12d}")
                output_parts.append(f"{status_color}{status:^12}{reset_color}")
                
                print(" | ".join(output_parts))
                
                # è®°å½•ç»Ÿè®¡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                all_ep_r[k].append(reward_total)
                all_ep_r0[k].append(reward_leaders[0])
                if self.n_follower > 0:
                    all_ep_r1[k].append(reward_followers[0])
                
                # ä¿å­˜æ¨¡å‹
                self._save_models(actors, episode)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        data = self._save_training_data(all_ep_r, all_ep_r0, all_ep_r1)
        
        # ç»˜åˆ¶ç»“æœ
        self._plot_results(data)
        
        # å…³é—­æ—¥å¿—
        self._close_logger()
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        return data

