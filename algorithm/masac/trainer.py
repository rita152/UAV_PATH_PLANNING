"""
MASAC è®­ç»ƒå™¨
è´Ÿè´£ SAC ç®—æ³•çš„è®­ç»ƒæµç¨‹æ§åˆ¶ã€æ¨¡å‹ä¿å­˜å’Œæ•°æ®è®°å½•
"""
import torch
import numpy as np
from matplotlib import pyplot as plt
import pickle as pkl
from utils import get_model_path, get_data_path, set_global_seed, get_episode_seed, print_seed_info
from .agent import Actor, Critic, Entropy
from .buffer import Memory
from .noise import Ornstein_Uhlenbeck_Noise


class Trainer:
    """
    MASAC è®­ç»ƒå™¨
    
    èŒè´£åˆ†ç¦»è®¾è®¡ï¼š
    - __init__: æ¥å—é…ç½®å‚æ•°ï¼ˆç¯å¢ƒå®ä¾‹ã€ç½‘ç»œç»“æ„ã€ç®—æ³•è¶…å‚æ•°ã€æ™ºèƒ½ä½“æ•°é‡ï¼‰
    - train: æ¥å—è®­ç»ƒå‚æ•°ï¼ˆè®­ç»ƒè½®æ•°ã€æ¸²æŸ“ç­‰ï¼‰
    
    è¿™æ ·è®¾è®¡çš„å¥½å¤„ï¼š
    1. ä¸€ä¸ªè®­ç»ƒå™¨å¯¹åº”ä¸€ä¸ªç¯å¢ƒï¼Œç®€æ´æ˜äº†
    2. ç½‘ç»œç»“æ„åœ¨åˆå§‹åŒ–æ—¶ç¡®å®šï¼ˆn_leader, n_followerå†³å®šç½‘ç»œç»´åº¦ï¼‰
    3. trainæ–¹æ³•åªéœ€è¦æ§åˆ¶è®­ç»ƒæµç¨‹ï¼Œä¸éœ€è¦ä¼ é€’ç¯å¢ƒ
    4. é€‚åˆå¤§å¤šæ•°ä½¿ç”¨åœºæ™¯ï¼ˆå›ºå®šç¯å¢ƒè®­ç»ƒï¼‰
    
    Args (é…ç½®å‚æ•°):
        env: Gymç¯å¢ƒå®ä¾‹
        n_leader: Leaderæ•°é‡ï¼ˆå†³å®šç½‘ç»œç»“æ„ï¼‰
        n_follower: Followeræ•°é‡ï¼ˆå†³å®šç½‘ç»œç»“æ„ï¼‰
        state_dim: çŠ¶æ€ç»´åº¦
        action_dim: åŠ¨ä½œç»´åº¦
        max_action: åŠ¨ä½œæœ€å¤§å€¼
        min_action: åŠ¨ä½œæœ€å°å€¼
        hidden_dim: ç½‘ç»œéšè—å±‚ç»´åº¦
        gamma: æŠ˜æ‰£å› å­
        q_lr: Qç½‘ç»œå­¦ä¹ ç‡
        value_lr: Valueç½‘ç»œå­¦ä¹ ç‡
        policy_lr: Policyå­¦ä¹ ç‡
        tau: è½¯æ›´æ–°ç³»æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        memory_capacity: ç»éªŒæ± å®¹é‡
        data_save_name: æ•°æ®ä¿å­˜æ–‡ä»¶å
    """
    def __init__(self, 
                 env,
                 n_leader,
                 n_follower,
                 state_dim,
                 action_dim,
                 max_action,
                 min_action,
                 hidden_dim=256,
                 gamma=0.9,
                 q_lr=3e-4,
                 value_lr=3e-3,
                 policy_lr=1e-3,
                 tau=1e-2,
                 batch_size=128,
                 memory_capacity=20000,
                 device='auto',
                 seed=42,
                 deterministic=False,
                 data_save_name='MASAC_new1.pkl'):
        
        # ç¯å¢ƒå®ä¾‹
        self.env = env
        
        # è®¾å¤‡é€‰æ‹©
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # æ‰“å°è®¾å¤‡ä¿¡æ¯
        self._print_device_info()
        
        # éšæœºç§å­ç®¡ç†
        self.base_seed = seed
        self.deterministic = deterministic
        
        # è®¾ç½®åˆå§‹å…¨å±€ç§å­
        set_global_seed(seed, deterministic)
        print_seed_info(seed, mode='train', deterministic=deterministic)
        
        # æ™ºèƒ½ä½“æ•°é‡ï¼ˆå†³å®šç½‘ç»œç»“æ„ï¼‰
        self.n_leader = n_leader
        self.n_follower = n_follower
        self.n_agents = n_leader + n_follower
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å‚æ•°
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = max_action
        self.min_action = min_action
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = hidden_dim
        self.q_lr = q_lr
        self.value_lr = value_lr
        self.policy_lr = policy_lr
        self.tau = tau
        
        # ç®—æ³•å‚æ•°
        self.gamma = gamma
        self.batch_size = batch_size
        self.memory_capacity = memory_capacity
        
        # æ•°æ®ä¿å­˜è·¯å¾„
        self.data_path = get_data_path(data_save_name)
    
    def _print_device_info(self):
        """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(self.device)
            gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9
            print(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print(f"ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ")
    
    def _initialize_agents(self):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“ç»„ä»¶ï¼ˆActor, Critic, Entropyï¼‰
        
        Returns:
            actors, critics, entropies: æ™ºèƒ½ä½“ç»„ä»¶åˆ—è¡¨
        """
        actors = []
        critics = []
        entropies = []
        
        for i in range(self.n_agents):
            # åˆ›å»º Actorï¼ˆç§»åˆ°GPUï¼‰
            actor = Actor(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                max_action=self.max_action,
                min_action=self.min_action,
                hidden_dim=self.hidden_dim,
                policy_lr=self.policy_lr,
                device=str(self.device)
            )
            actors.append(actor)
            
            # åˆ›å»º Criticï¼ˆç§»åˆ°GPUï¼‰
            critic = Critic(
                state_dim=self.state_dim * self.n_agents,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                value_lr=self.value_lr,
                tau=self.tau,
                device=str(self.device)
            )
            critics.append(critic)
            
            # åˆ›å»º Entropy è°ƒèŠ‚å™¨ï¼ˆç§»åˆ°GPUï¼‰
            entropy = Entropy(
                target_entropy=-0.1,
                lr=self.q_lr,
                device=str(self.device)
            )
            entropies.append(entropy)
        
        return actors, critics, entropies
    
    def _initialize_memory(self):
        """
        åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº
        
        Returns:
            memory: ç»éªŒå›æ”¾ç¼“å†²åŒº
        """
        transition_dim = (2 * self.state_dim * self.n_agents + 
                         self.action_dim * self.n_agents + 
                         1 * self.n_agents)
        return Memory(capacity=self.memory_capacity, transition_dim=transition_dim)
    
    def _initialize_noise(self):
        """
        åˆå§‹åŒ– OU å™ªå£°ç”Ÿæˆå™¨
        
        Returns:
            noise: OUå™ªå£°ç”Ÿæˆå™¨
        """
        return Ornstein_Uhlenbeck_Noise(
            mean=np.zeros((self.n_agents, self.action_dim)),
            sigma=0.1,
            theta=0.1,
            dt=1e-2
        )
    
    def _collect_experience(self, actors, observation, episode, ou_noise):
        """
        é‡‡é›†ç»éªŒï¼ˆé€‰æ‹©åŠ¨ä½œå¹¶æ·»åŠ å™ªå£°ï¼‰
        
        Args:
            actors: Actoråˆ—è¡¨
            observation: å½“å‰è§‚æµ‹
            episode: å½“å‰è½®æ•°
            ou_noise: OUå™ªå£°ç”Ÿæˆå™¨
            
        Returns:
            action: æ‰§è¡Œçš„åŠ¨ä½œ
        """
        action = np.zeros((self.n_agents, self.action_dim))
        
        # é€‰æ‹©åŠ¨ä½œ
        for i in range(self.n_agents):
            action[i] = actors[i].choose_action(observation[i])
        
        # å‰20è½®æ·»åŠ  OU å™ªå£°è¿›è¡Œæ¢ç´¢
        if episode <= 20:
            noise = ou_noise()
        else:
            noise = 0
        
        action = action + noise
        action = np.clip(action, -self.max_action, self.max_action)
        
        return action
    
    def _update_agents(self, actors, critics, entropies, memory):
        """
        æ›´æ–°æ™ºèƒ½ä½“ç½‘ç»œå‚æ•°
        
        Args:
            actors: Actoråˆ—è¡¨
            critics: Criticåˆ—è¡¨
            entropies: Entropyåˆ—è¡¨
            memory: ç»éªŒå›æ”¾ç¼“å†²åŒº
        """
        # ä»ç»éªŒæ± é‡‡æ ·ï¼ˆCPUæ•°æ®ï¼‰
        b_M = memory.sample(self.batch_size)
        b_s = b_M[:, :self.state_dim * self.n_agents]
        b_a = b_M[:, self.state_dim * self.n_agents : 
                     self.state_dim * self.n_agents + self.action_dim * self.n_agents]
        b_r = b_M[:, -self.state_dim * self.n_agents - 1 * self.n_agents : 
                     -self.state_dim * self.n_agents]
        b_s_ = b_M[:, -self.state_dim * self.n_agents:]
        
        # è½¬æ¢ä¸º Tensor å¹¶ç§»åˆ° GPU
        b_s = torch.FloatTensor(b_s).to(self.device)
        b_a = torch.FloatTensor(b_a).to(self.device)
        b_r = torch.FloatTensor(b_r).to(self.device)
        b_s_ = torch.FloatTensor(b_s_).to(self.device)
        
        # æ›´æ–°æ¯ä¸ªæ™ºèƒ½ä½“
        for i in range(self.n_agents):
            # è®¡ç®—ç›®æ ‡ Q å€¼
            new_action, log_prob_ = actors[i].evaluate(
                b_s_[:, self.state_dim * i : self.state_dim * (i + 1)]
            )
            target_q1, target_q2 = critics[i].get_target_q_value(b_s_, new_action)
            target_q = b_r[:, i:(i + 1)] + self.gamma * (
                torch.min(target_q1, target_q2) - 
                entropies[i].alpha * log_prob_.sum(dim=-1, keepdim=True)
            )
            
            # æ›´æ–° Critic
            current_q1, current_q2 = critics[i].get_q_value(
                b_s, b_a[:, self.action_dim * i : self.action_dim * (i + 1)]
            )
            critics[i].update(current_q1, current_q2, target_q.detach())
            
            # æ›´æ–° Actor
            a, log_prob = actors[i].evaluate(
                b_s[:, self.state_dim * i : self.state_dim * (i + 1)]
            )
            q1, q2 = critics[i].get_q_value(b_s, a)
            q = torch.min(q1, q2)
            actor_loss = (entropies[i].alpha * log_prob.sum(dim=-1, keepdim=True) - q).mean()
            actors[i].update(actor_loss)
            
            # æ›´æ–° Entropy
            alpha_loss = -(entropies[i].log_alpha.exp() * (
                log_prob.sum(dim=-1, keepdim=True) + entropies[i].target_entropy
            ).detach()).mean()
            entropies[i].update(alpha_loss)
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            critics[i].soft_update()
    
    def _save_models(self, actors, episode):
        """
        ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆè‡ªåŠ¨å¤„ç†GPU/CPUï¼‰
        åªä¿å­˜ä¸¤ä¸ªæ–‡ä»¶ï¼šleader.pth å’Œ follower.pth
        follower.pth åŒ…å«æ‰€æœ‰Followerçš„ç‹¬ç«‹æƒé‡
        
        Args:
            actors: Actoråˆ—è¡¨
            episode: å½“å‰è½®æ•°
        """
        if episode % 20 == 0 and episode > 200:
            # ä¿å­˜ Leader æ¨¡å‹ï¼ˆæ‰€æœ‰Leaderçš„æƒé‡ï¼‰
            leader_save_data = {}
            for i in range(self.n_leader):
                leader_save_data[f'leader_{i}'] = {
                    'net': actors[i].action_net.cpu().state_dict(),
                    'opt': actors[i].optimizer.state_dict()
                }
            torch.save(leader_save_data, get_model_path('leader.pth'))
            
            # ä¿å­˜ Follower æ¨¡å‹ï¼ˆæ‰€æœ‰Followerçš„æƒé‡æ‰“åŒ…åˆ°ä¸€ä¸ªæ–‡ä»¶ï¼‰
            if self.n_follower > 0:
                follower_save_data = {}
                for j in range(self.n_follower):
                    follower_idx = self.n_leader + j
                    follower_save_data[f'follower_{j}'] = {
                        'net': actors[follower_idx].action_net.cpu().state_dict(),
                        'opt': actors[follower_idx].optimizer.state_dict()
                    }
                torch.save(follower_save_data, get_model_path('follower.pth'))
            
            # ä¿å­˜åç§»å›GPU
            for i in range(self.n_leader):
                actors[i].action_net.to(self.device)
            for j in range(self.n_follower):
                actors[self.n_leader + j].action_net.to(self.device)
    
    def _save_training_data(self, all_ep_r, all_ep_r0, all_ep_r1):
        """
        ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆå¥–åŠ±ç»Ÿè®¡ï¼‰
        
        Args:
            all_ep_r: æ€»å¥–åŠ±åˆ—è¡¨
            all_ep_r0: Leaderå¥–åŠ±åˆ—è¡¨
            all_ep_r1: Followerå¥–åŠ±åˆ—è¡¨
            
        Returns:
            data: ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        all_ep_r_mean = np.mean(np.array(all_ep_r), axis=0)
        all_ep_r_std = np.std(np.array(all_ep_r), axis=0)
        all_ep_L_mean = np.mean(np.array(all_ep_r0), axis=0)
        all_ep_L_std = np.std(np.array(all_ep_r0), axis=0)
        all_ep_F_mean = np.mean(np.array(all_ep_r1), axis=0)
        all_ep_F_std = np.std(np.array(all_ep_r1), axis=0)
        
        data = {
            "all_ep_r_mean": all_ep_r_mean,
            "all_ep_r_std": all_ep_r_std,
            "all_ep_L_mean": all_ep_L_mean,
            "all_ep_L_std": all_ep_L_std,
            "all_ep_F_mean": all_ep_F_mean,
            "all_ep_F_std": all_ep_F_std,
        }
        
        with open(self.data_path, 'wb') as f:
            pkl.dump(data, f, pkl.HIGHEST_PROTOCOL)
        
        return data
    
    def _plot_results(self, data):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        
        Args:
            data: è®­ç»ƒç»Ÿè®¡æ•°æ®å­—å…¸
        """
        all_ep_r_mean = data["all_ep_r_mean"]
        all_ep_r_std = data["all_ep_r_std"]
        all_ep_L_mean = data["all_ep_L_mean"]
        all_ep_L_std = data["all_ep_L_std"]
        all_ep_F_mean = data["all_ep_F_mean"]
        all_ep_F_std = data["all_ep_F_std"]
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        all_ep_r_max = all_ep_r_mean + all_ep_r_std * 0.95
        all_ep_r_min = all_ep_r_mean - all_ep_r_std * 0.95
        all_ep_L_max = all_ep_L_mean + all_ep_L_std * 0.95
        all_ep_L_min = all_ep_L_mean - all_ep_L_std * 0.95
        all_ep_F_max = all_ep_F_mean + all_ep_F_std * 0.95
        all_ep_F_min = all_ep_F_mean - all_ep_F_std * 0.95
        
        # ç»˜åˆ¶æ€»å¥–åŠ±æ›²çº¿
        plt.figure(1)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_r_mean)), all_ep_r_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_r_mean)), all_ep_r_max, all_ep_r_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Total reward')
        
        # ç»˜åˆ¶ Leader å¥–åŠ±æ›²çº¿
        plt.figure(2, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_L_mean)), all_ep_L_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_L_mean)), all_ep_L_max, all_ep_L_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Leader reward')
        
        # ç»˜åˆ¶ Follower å¥–åŠ±æ›²çº¿
        plt.figure(3, figsize=(8, 4), dpi=150)
        plt.margins(x=0)
        plt.plot(np.arange(len(all_ep_F_mean)), all_ep_F_mean, 
                label='MASAC', color='#e75840')
        plt.fill_between(np.arange(len(all_ep_F_mean)), all_ep_F_max, all_ep_F_min, 
                         alpha=0.6, facecolor='#e75840')
        plt.xlabel('Episode')
        plt.ylabel('Follower reward')
        plt.legend()
        plt.show()
    
    def train(self, ep_max=500, ep_len=1000, train_num=1, render=False):
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Args (è®­ç»ƒå‚æ•°):
            ep_max: æœ€å¤§è®­ç»ƒè½®æ•°
            ep_len: æ¯è½®æœ€å¤§æ­¥æ•°
            train_num: è®­ç»ƒæ¬¡æ•°ï¼ˆç”¨äºå¤šæ¬¡å®éªŒï¼‰
            render: æ˜¯å¦æ¸²æŸ“
            
        Returns:
            data: è®­ç»ƒç»Ÿè®¡æ•°æ®å­—å…¸
        """
        print('SACè®­ç»ƒä¸­...')
        
        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
        all_ep_r = [[] for _ in range(train_num)]
        all_ep_r0 = [[] for _ in range(train_num)]
        all_ep_r1 = [[] for _ in range(train_num)]
        
        for k in range(train_num):
            # åˆå§‹åŒ–ç»„ä»¶
            actors, critics, entropies = self._initialize_agents()
            memory = self._initialize_memory()
            ou_noise = self._initialize_noise()
            
            # æ‰“å°è¡¨å¤´
            print("\n" + "="*80)
            header_parts = ["Episode"]
            for i in range(self.n_leader):
                header_parts.append(f"Leader{i}")
            for j in range(self.n_follower):
                header_parts.append(f"Follower{j}")
            header_parts.append("Steps")
            header_parts.append("Status")
            print(" | ".join([f"{part:^12}" for part in header_parts]))
            print("="*80)
            
            # è®­ç»ƒå¾ªç¯
            for episode in range(ep_max):
                # ä¸ºæ¯ä¸ªepisodeè®¾ç½®ä¸åŒçš„ç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
                episode_seed = get_episode_seed(self.base_seed, episode, mode='train')
                set_global_seed(episode_seed, self.deterministic)
                
                observation = self.env.reset()
                reward_total = 0
                
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬ç»Ÿè®¡å¥–åŠ±
                reward_leaders = [0.0] * self.n_leader
                reward_followers = [0.0] * self.n_follower
                
                for timestep in range(ep_len):
                    # é‡‡é›†ç»éªŒ
                    action = self._collect_experience(actors, observation, episode, ou_noise)
                    
                    # ç¯å¢ƒäº¤äº’
                    observation_, reward, done, win, team_counter = self.env.step(action)
                    
                    # å­˜å‚¨ç»éªŒ
                    memory.store(observation.flatten(), action.flatten(), 
                               reward.flatten(), observation_.flatten())
                    
                    # å­¦ä¹ æ›´æ–°
                    if memory.counter > self.memory_capacity:
                        self._update_agents(actors, critics, entropies, memory)
                    
                    # æ›´æ–°çŠ¶æ€
                    observation = observation_
                    reward_total += reward.mean()
                    
                    # åˆ†åˆ«ç´¯ç§¯æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±
                    for i in range(self.n_leader):
                        reward_leaders[i] += float(reward[i])
                    for j in range(self.n_follower):
                        reward_followers[j] += float(reward[self.n_leader + j])
                    
                    # æ¸²æŸ“
                    if render:
                        self.env.render()
                    
                    # æ£€æŸ¥ç»ˆæ­¢
                    if done:
                        break
                
                # åˆ¤æ–­ç»ˆæ­¢çŠ¶æ€
                if done:
                    if win:
                        status = "âœ… Success"
                        status_color = "\033[92m"  # ç»¿è‰²
                    else:
                        status = "âŒ Failure"
                        status_color = "\033[91m"  # çº¢è‰²
                else:
                    status = "â±ï¸  Timeout"
                    status_color = "\033[93m"  # é»„è‰²
                reset_color = "\033[0m"
                
                # æ ¼å¼åŒ–è¾“å‡º
                output_parts = [f"{episode:^12d}"]
                
                # Leaderå¥–åŠ±
                for i in range(self.n_leader):
                    output_parts.append(f"{reward_leaders[i]:^12.2f}")
                
                # Followerå¥–åŠ±
                for j in range(self.n_follower):
                    output_parts.append(f"{reward_followers[j]:^12.2f}")
                
                # æ­¥æ•°å’ŒçŠ¶æ€
                output_parts.append(f"{timestep+1:^12d}")
                output_parts.append(f"{status_color}{status:^12}{reset_color}")
                
                print(" | ".join(output_parts))
                
                # è®°å½•ç»Ÿè®¡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                all_ep_r[k].append(reward_total)
                all_ep_r0[k].append(reward_leaders[0])
                if self.n_follower > 0:
                    all_ep_r1[k].append(reward_followers[0])
                
                # ä¿å­˜æ¨¡å‹
                self._save_models(actors, episode)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        data = self._save_training_data(all_ep_r, all_ep_r0, all_ep_r1)
        
        # ç»˜åˆ¶ç»“æœ
        self._plot_results(data)
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        return data

