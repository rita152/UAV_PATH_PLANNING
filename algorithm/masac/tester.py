"""
MASAC æµ‹è¯•å™¨
è´Ÿè´£åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¿›è¡Œæ€§èƒ½è¯„ä¼°
"""
import torch
import numpy as np
from utils import get_model_path, set_global_seed, get_episode_seed, print_seed_info
from .agent import Actor


class Tester:
    """
    MASAC æµ‹è¯•å™¨
    
    èŒè´£åˆ†ç¦»è®¾è®¡ï¼š
    - __init__: æ¥å—é…ç½®å‚æ•°ï¼ˆç¯å¢ƒå®ä¾‹ã€ç½‘ç»œç»“æ„ã€æ¨¡å‹è·¯å¾„ã€æ™ºèƒ½ä½“æ•°é‡ï¼‰
    - test: æ¥å—æµ‹è¯•å‚æ•°ï¼ˆæµ‹è¯•è½®æ•°ã€æ¸²æŸ“ç­‰ï¼‰
    
    è¿™æ ·è®¾è®¡çš„å¥½å¤„ï¼š
    1. ä¸€ä¸ªæµ‹è¯•å™¨å¯¹åº”ä¸€ä¸ªç¯å¢ƒï¼Œç®€æ´æ˜äº†
    2. ç½‘ç»œç»“æ„åœ¨åˆå§‹åŒ–æ—¶ç¡®å®šï¼ˆn_leader, n_followerå†³å®šæµ‹è¯•æµç¨‹ï¼‰
    3. testæ–¹æ³•åªéœ€è¦æ§åˆ¶æµ‹è¯•æµç¨‹ï¼Œä¸éœ€è¦ä¼ é€’ç¯å¢ƒ
    4. é€‚åˆå¤§å¤šæ•°ä½¿ç”¨åœºæ™¯ï¼ˆå›ºå®šç¯å¢ƒæµ‹è¯•ï¼‰
    
    Args (é…ç½®å‚æ•°):
        env: Gymç¯å¢ƒå®ä¾‹
        n_leader: Leaderæ•°é‡ï¼ˆå†³å®šæµ‹è¯•æµç¨‹ï¼‰
        n_follower: Followeræ•°é‡ï¼ˆå†³å®šæµ‹è¯•æµç¨‹ï¼‰
        state_dim: çŠ¶æ€ç»´åº¦
        action_dim: åŠ¨ä½œç»´åº¦
        max_action: åŠ¨ä½œæœ€å¤§å€¼
        min_action: åŠ¨ä½œæœ€å°å€¼
        hidden_dim: ç½‘ç»œéšè—å±‚ç»´åº¦
        policy_lr: Policyå­¦ä¹ ç‡
        leader_model_path: Leaderæ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨è·å–ï¼‰
        follower_model_path: Followeræ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨è·å–ï¼‰
    """
    def __init__(self,
                 env,
                 n_leader,
                 n_follower,
                 state_dim,
                 action_dim,
                 max_action,
                 min_action,
                 hidden_dim=256,
                 policy_lr=1e-3,
                 device='auto',
                 seed=42,
                 leader_model_path=None,
                 follower_model_path=None):
        
        # ç¯å¢ƒå®ä¾‹
        self.env = env
        
        # è®¾å¤‡é€‰æ‹©
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # æ‰“å°è®¾å¤‡ä¿¡æ¯
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(self.device)
            print(f"ğŸš€ ä½¿ç”¨GPUæµ‹è¯•: {gpu_name}")
        else:
            print(f"ğŸ’» ä½¿ç”¨CPUæµ‹è¯•")
        
        # éšæœºç§å­ç®¡ç†ï¼ˆæµ‹è¯•ä½¿ç”¨ä¸åŒçš„ç§å­ç©ºé—´ï¼‰
        self.base_seed = seed
        
        # è®¾ç½®åˆå§‹å…¨å±€ç§å­
        set_global_seed(seed, deterministic=False)
        print_seed_info(seed, mode='test', deterministic=False)
        
        # æ™ºèƒ½ä½“æ•°é‡
        self.n_leader = n_leader
        self.n_follower = n_follower
        self.n_agents = n_leader + n_follower
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å‚æ•°
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = max_action
        self.min_action = min_action
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = hidden_dim
        self.policy_lr = policy_lr
        
        # æ¨¡å‹è·¯å¾„ï¼ˆæ‰€æœ‰Followerå…±äº«åŒä¸€ä¸ªæƒé‡æ–‡ä»¶ï¼‰
        self.leader_model_path = leader_model_path or get_model_path('leader.pth')
        self.follower_model_path = follower_model_path or get_model_path('follower.pth')
    
    def _load_actors(self):
        """
        åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“çš„æ¨¡å‹
        leader.pth åŒ…å«æ‰€æœ‰Leaderçš„æƒé‡
        follower.pth åŒ…å«æ‰€æœ‰Followerçš„æƒé‡
        
        Returns:
            actors: Actoråˆ—è¡¨ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ï¼‰
        """
        actors = []
        
        # åŠ è½½Leaderæ¨¡å‹
        leader_checkpoint = torch.load(self.leader_model_path, map_location=self.device)
        
        for i in range(self.n_leader):
            actor = Actor(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                max_action=self.max_action,
                min_action=self.min_action,
                hidden_dim=self.hidden_dim,
                policy_lr=self.policy_lr,
                device=str(self.device)
            )
            # åŠ è½½å¯¹åº”Leaderçš„æƒé‡
            actor.action_net.load_state_dict(leader_checkpoint[f'leader_{i}']['net'])
            actors.append(actor)
        
        # åŠ è½½Followeræ¨¡å‹
        if self.n_follower > 0:
            follower_checkpoint = torch.load(self.follower_model_path, map_location=self.device)
            
            for j in range(self.n_follower):
                actor = Actor(
                    state_dim=self.state_dim,
                    action_dim=self.action_dim,
                    max_action=self.max_action,
                    min_action=self.min_action,
                    hidden_dim=self.hidden_dim,
                    policy_lr=self.policy_lr,
                    device=str(self.device)
                )
                # åŠ è½½å¯¹åº”Followerçš„æƒé‡
                actor.action_net.load_state_dict(follower_checkpoint[f'follower_{j}']['net'])
                actors.append(actor)
        
        return actors
    
    def _select_actions(self, actors, state):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥ï¼Œæ— æ¢ç´¢å™ªå£°ï¼‰
        æ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨è‡ªå·±ç‹¬ç«‹çš„æƒé‡
        
        Args:
            actors: Actoråˆ—è¡¨ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ï¼‰
            state: å½“å‰çŠ¶æ€
            
        Returns:
            action: åŠ¨ä½œæ•°ç»„
        """
        action = np.zeros((self.n_agents, self.action_dim))
        
        # æ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨è‡ªå·±çš„ç­–ç•¥ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
        for i in range(self.n_agents):
            action[i] = actors[i].choose_action(state[i])
        
        return action
    
    def test(self, ep_len=1000, test_episode=100, render=False):
        """
        æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹
        
        Args (æµ‹è¯•å‚æ•°):
            ep_len: æ¯è½®æœ€å¤§æ­¥æ•°
            test_episode: æµ‹è¯•è½®æ•°
            render: æ˜¯å¦æ¸²æŸ“
            
        Returns:
            results: æµ‹è¯•ç»“æœå­—å…¸
        """
        print('SACæµ‹è¯•ä¸­...')
        
        # åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“çš„æ¨¡å‹ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹æƒé‡ï¼‰
        actors = self._load_actors()
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        win_times = 0
        average_FKR = 0
        average_timestep = 0
        average_integral_V = 0
        average_integral_U = 0
        all_ep_V = []
        all_ep_U = []
        all_ep_T = []
        all_ep_F = []
        
        # æµ‹è¯•å¾ªç¯
        for j in range(test_episode):
            # ä¸ºæ¯ä¸ªepisodeè®¾ç½®ä¸åŒçš„ç§å­ï¼ˆæµ‹è¯•ç§å­ç©ºé—´ï¼‰
            episode_seed = get_episode_seed(self.base_seed, j, mode='test')
            set_global_seed(episode_seed, deterministic=False)
            
            state = self.env.reset()
            total_rewards = 0
            integral_V = 0
            integral_U = 0
            v, v1 = [], []
            
            for timestep in range(ep_len):
                # é€‰æ‹©åŠ¨ä½œï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨è‡ªå·±çš„æƒé‡ï¼‰
                action = self._select_actions(actors, state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                new_state, reward, done, win, team_counter = self.env.step(action)
                
                # è®°å½•èƒœåˆ©
                if win:
                    win_times += 1
                
                # è®°å½•æ•°æ®
                v.append(state[0][2] * 30)
                v1.append(state[1][2] * 30)
                integral_V += state[0][2]
                integral_U += abs(action[0]).sum()
                total_rewards += reward.mean()
                
                # æ›´æ–°çŠ¶æ€
                state = new_state
                
                # æ¸²æŸ“
                if render:
                    self.env.render()
                
                # æ£€æŸ¥ç»ˆæ­¢
                if done:
                    break
            
            # æ›´æ–°ç»Ÿè®¡
            FKR = team_counter / timestep if timestep > 0 else 0
            average_FKR += FKR
            average_timestep += timestep
            average_integral_V += integral_V
            average_integral_U += integral_U
            all_ep_V.append(integral_V)
            all_ep_U.append(integral_U)
            all_ep_T.append(timestep)
            all_ep_F.append(FKR)
            
            print("Score", total_rewards)
        
        # æ‰“å°ç»“æœ
        print('ä»»åŠ¡å®Œæˆç‡', win_times / test_episode)
        print('å¹³å‡æœ€å¤§ç¼–é˜Ÿä¿æŒç‡', average_FKR / test_episode)
        print('å¹³å‡æœ€çŸ­é£è¡Œæ—¶é—´', average_timestep / test_episode)
        print('å¹³å‡æœ€çŸ­é£è¡Œè·¯ç¨‹', average_integral_V / test_episode)
        print('å¹³å‡æœ€å°èƒ½é‡æŸè€—', average_integral_U / test_episode)
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        # è¿”å›ç»“æœ
        results = {
            'win_rate': win_times / test_episode,
            'average_FKR': average_FKR / test_episode,
            'average_timestep': average_timestep / test_episode,
            'average_integral_V': average_integral_V / test_episode,
            'average_integral_U': average_integral_U / test_episode,
            'all_ep_V': all_ep_V,
            'all_ep_U': all_ep_U,
            'all_ep_T': all_ep_T,
            'all_ep_F': all_ep_F,
        }
        
        return results

