"""
MASAC æµ‹è¯•å™¨
è´Ÿè´£åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¿›è¡Œæ€§èƒ½è¯„ä¼°
"""
import torch
import numpy as np
from utils import get_model_path, set_global_seed, get_episode_seed, print_seed_info
from .agent import Actor


class Tester:
    """
    MASAC æµ‹è¯•å™¨
    
    èŒè´£åˆ†ç¦»è®¾è®¡ï¼š
    - __init__: æ¥å—é…ç½®å‚æ•°ï¼ˆç¯å¢ƒå®ä¾‹ã€ç½‘ç»œç»“æ„ã€æ¨¡å‹è·¯å¾„ã€æ™ºèƒ½ä½“æ•°é‡ï¼‰
    - test: æ¥å—æµ‹è¯•å‚æ•°ï¼ˆæµ‹è¯•è½®æ•°ã€æ¸²æŸ“ç­‰ï¼‰
    
    è¿™æ ·è®¾è®¡çš„å¥½å¤„ï¼š
    1. ä¸€ä¸ªæµ‹è¯•å™¨å¯¹åº”ä¸€ä¸ªç¯å¢ƒï¼Œç®€æ´æ˜äº†
    2. ç½‘ç»œç»“æ„åœ¨åˆå§‹åŒ–æ—¶ç¡®å®šï¼ˆn_leader, n_followerå†³å®šæµ‹è¯•æµç¨‹ï¼‰
    3. testæ–¹æ³•åªéœ€è¦æ§åˆ¶æµ‹è¯•æµç¨‹ï¼Œä¸éœ€è¦ä¼ é€’ç¯å¢ƒ
    4. é€‚åˆå¤§å¤šæ•°ä½¿ç”¨åœºæ™¯ï¼ˆå›ºå®šç¯å¢ƒæµ‹è¯•ï¼‰
    
    Args (é…ç½®å‚æ•°):
        env: Gymç¯å¢ƒå®ä¾‹
        n_leader: Leaderæ•°é‡ï¼ˆå†³å®šæµ‹è¯•æµç¨‹ï¼‰
        n_follower: Followeræ•°é‡ï¼ˆå†³å®šæµ‹è¯•æµç¨‹ï¼‰
        state_dim: çŠ¶æ€ç»´åº¦
        action_dim: åŠ¨ä½œç»´åº¦
        max_action: åŠ¨ä½œæœ€å¤§å€¼
        min_action: åŠ¨ä½œæœ€å°å€¼
        hidden_dim: ç½‘ç»œéšè—å±‚ç»´åº¦
        policy_lr: Policyå­¦ä¹ ç‡
        leader_model_path: Leaderæ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨è·å–ï¼‰
        follower_model_path: Followeræ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨è·å–ï¼‰
    """
    def __init__(self,
                 env,
                 n_leader,
                 n_follower,
                 state_dim,
                 action_dim,
                 max_action,
                 min_action,
                 hidden_dim=256,
                 policy_lr=1e-3,
                 device='auto',
                 seed=42,
                 leader_model_path=None,
                 follower_model_path=None):
        
        # ç¯å¢ƒå®ä¾‹
        self.env = env
        
        # è®¾å¤‡é€‰æ‹©
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # æ‰“å°è®¾å¤‡ä¿¡æ¯
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(self.device)
            print(f"ğŸš€ ä½¿ç”¨GPUæµ‹è¯•: {gpu_name}")
        else:
            print(f"ğŸ’» ä½¿ç”¨CPUæµ‹è¯•")
        
        # éšæœºç§å­ç®¡ç†ï¼ˆæµ‹è¯•ä½¿ç”¨ä¸åŒçš„ç§å­ç©ºé—´ï¼‰
        self.base_seed = seed
        
        # è®¾ç½®åˆå§‹å…¨å±€ç§å­
        set_global_seed(seed, deterministic=False)
        print_seed_info(seed, mode='test', deterministic=False)
        
        # æ™ºèƒ½ä½“æ•°é‡
        self.n_leader = n_leader
        self.n_follower = n_follower
        self.n_agents = n_leader + n_follower
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å‚æ•°
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = max_action
        self.min_action = min_action
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = hidden_dim
        self.policy_lr = policy_lr
        
        # æ¨¡å‹è·¯å¾„ï¼ˆæ‰€æœ‰Followerå…±äº«åŒä¸€ä¸ªæƒé‡æ–‡ä»¶ï¼‰
        self.leader_model_path = leader_model_path or get_model_path('leader.pth')
        self.follower_model_path = follower_model_path or get_model_path('follower.pth')
    
    def _load_actors(self):
        """
        åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“çš„æ¨¡å‹
        leader.pth åŒ…å«æ‰€æœ‰Leaderçš„æƒé‡
        follower.pth åŒ…å«æ‰€æœ‰Followerçš„æƒé‡
        
        Returns:
            actors: Actoråˆ—è¡¨ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ï¼‰
        """
        actors = []
        
        # åŠ è½½Leaderæ¨¡å‹
        leader_checkpoint = torch.load(self.leader_model_path, map_location=self.device)
        
        for i in range(self.n_leader):
            actor = Actor(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                max_action=self.max_action,
                min_action=self.min_action,
                hidden_dim=self.hidden_dim,
                policy_lr=self.policy_lr,
                device=str(self.device)
            )
            # åŠ è½½å¯¹åº”Leaderçš„æƒé‡ï¼ˆé€‚é…æ–°çš„ä¿å­˜æ ¼å¼ï¼‰
            checkpoint_data = leader_checkpoint[f'leader_{i}']
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆ'net'ï¼‰å’Œæ–°æ ¼å¼ï¼ˆ'actor_net'ï¼‰
            if 'actor_net' in checkpoint_data:
                actor.action_net.load_state_dict(checkpoint_data['actor_net'])
            else:
                actor.action_net.load_state_dict(checkpoint_data['net'])
            actors.append(actor)
        
        # åŠ è½½Followeræ¨¡å‹
        if self.n_follower > 0:
            follower_checkpoint = torch.load(self.follower_model_path, map_location=self.device)
            
            for j in range(self.n_follower):
                actor = Actor(
                    state_dim=self.state_dim,
                    action_dim=self.action_dim,
                    max_action=self.max_action,
                    min_action=self.min_action,
                    hidden_dim=self.hidden_dim,
                    policy_lr=self.policy_lr,
                    device=str(self.device)
                )
                # åŠ è½½å¯¹åº”Followerçš„æƒé‡ï¼ˆé€‚é…æ–°çš„ä¿å­˜æ ¼å¼ï¼‰
                checkpoint_data = follower_checkpoint[f'follower_{j}']
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆ'net'ï¼‰å’Œæ–°æ ¼å¼ï¼ˆ'actor_net'ï¼‰
                if 'actor_net' in checkpoint_data:
                    actor.action_net.load_state_dict(checkpoint_data['actor_net'])
                else:
                    actor.action_net.load_state_dict(checkpoint_data['net'])
                actors.append(actor)
        
        return actors
    
    def _select_actions(self, actors, state):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¿›è¡Œæµ‹è¯•ï¼‰- ä½¿ç”¨æ‰¹é‡å¤„ç†ä¼˜åŒ–CPU-GPUä¼ è¾“
        
        æ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨è‡ªå·±ç‹¬ç«‹çš„æƒé‡
        
        ä¼˜åŒ–è¯´æ˜ï¼š
        - ä½¿ç”¨æ‰¹é‡æ–¹æ³•ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰agentçš„åŠ¨ä½œ
        - å‡å°‘CPU-GPUæ•°æ®ä¼ è¾“æ¬¡æ•°ï¼ˆä» 2*n_agents æ¬¡é™ä½åˆ° 2 æ¬¡ï¼‰
        - æ˜¾è‘—æå‡æµ‹è¯•é€Ÿåº¦ï¼ˆç‰¹åˆ«æ˜¯å¤šagentåœºæ™¯ï¼‰
        
        Args:
            actors: Actoråˆ—è¡¨ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ï¼‰
            state: å½“å‰çŠ¶æ€ [n_agents, state_dim]
            
        Returns:
            action: åŠ¨ä½œæ•°ç»„ [n_agents, action_dim]
        """
        # ä½¿ç”¨æ‰¹é‡ç¡®å®šæ€§æ–¹æ³•ï¼ˆä¼˜åŒ–ï¼šå‡å°‘CPU-GPUä¼ è¾“ï¼‰
        action = Actor.choose_actions_batch_deterministic(actors, state, self.device)
        
        return action
    
    def test(self, ep_len=1000, test_episode=100, render=False):
        """
        æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹

        Args (æµ‹è¯•å‚æ•°):
            ep_len: æ¯è½®æœ€å¤§æ­¥æ•°
            test_episode: æµ‹è¯•è½®æ•°
            render: æ˜¯å¦æ¸²æŸ“

        Returns:
            results: æµ‹è¯•ç»“æœå­—å…¸
        """
        
        # åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“çš„æ¨¡å‹ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹æƒé‡ï¼‰
        actors = self._load_actors()
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        win_times = 0
        average_FKR = 0
        average_timestep = 0
        average_integral_V = 0
        average_integral_U = 0
        all_ep_V = []
        all_ep_U = []
        all_ep_T = []
        all_ep_F = []
        all_win = []  # è®°å½•æ¯ä¸ªepisodeçš„èƒœåˆ©æƒ…å†µ
        
        # æµ‹è¯•å¾ªç¯
        for j in range(test_episode):
            # ä¸ºæ¯ä¸ªepisodeè®¾ç½®ä¸åŒçš„ç§å­ï¼ˆæµ‹è¯•ç§å­ç©ºé—´ï¼‰
            episode_seed = get_episode_seed(self.base_seed, j, mode='test')
            set_global_seed(episode_seed, deterministic=False)

            # é‡ç½®ç¯å¢ƒï¼ˆç¬¦åˆ Gymnasium æ ‡å‡†ï¼‰
            state, reset_info = self.env.reset()
            total_rewards = 0
            integral_V = 0
            integral_U = 0
            v, v1 = [], []

            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬ç»Ÿè®¡å¥–åŠ±
            reward_leaders = [0.0] * self.n_leader
            reward_followers = [0.0] * self.n_follower
            
            for timestep in range(ep_len):
                # é€‰æ‹©åŠ¨ä½œï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨è‡ªå·±çš„æƒé‡ï¼‰
                action = self._select_actions(actors, state)
                
                # æ‰§è¡ŒåŠ¨ä½œï¼ˆç¬¦åˆ Gymnasium æ ‡å‡†ï¼‰
                new_state, reward, terminated, truncated, info = self.env.step(action)
                
                # ä» info ä¸­æå–é¢å¤–ä¿¡æ¯
                win = info['win']
                team_counter = info['team_counter']
                done = terminated or truncated
                
                # è®°å½•èƒœåˆ©
                if win:
                    win_times += 1
                
                # è®°å½•æ•°æ®
                v.append(state[0][2] * 30)
                v1.append(state[1][2] * 30)
                integral_V += state[0][2]
                integral_U += abs(action[0]).sum()
                total_rewards += reward.mean()

                # åˆ†åˆ«ç´¯ç§¯æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±
                for i in range(self.n_leader):
                    reward_leaders[i] += float(reward[i])
                for k in range(self.n_follower):
                    reward_followers[k] += float(reward[self.n_leader + k])

                # æ›´æ–°çŠ¶æ€
                state = new_state
                
                # æ¸²æŸ“
                if render:
                    self.env.render()
                
                # æ£€æŸ¥ç»ˆæ­¢
                if done:
                    break
            
            # æ›´æ–°ç»Ÿè®¡ï¼ˆä¿®å¤ï¼štimestepæ˜¯ç´¢å¼•ï¼Œæ€»æ­¥æ•°æ˜¯timestep+1ï¼‰
            total_steps = timestep + 1
            FKR = team_counter / total_steps if total_steps > 0 else 0
            average_FKR += FKR
            average_timestep += total_steps
            average_integral_V += integral_V
            average_integral_U += integral_U
            all_ep_V.append(integral_V)
            all_ep_U.append(integral_U)
            all_ep_T.append(total_steps)
            all_ep_F.append(FKR)
            all_win.append(win)  # è®°å½•èƒœåˆ©æƒ…å†µ

            # åˆ¤æ–­ç»ˆæ­¢çŠ¶æ€
            if done:
                if win:
                    status = "âœ… Success"
                    status_color = "\033[92m"  # ç»¿è‰²
                else:
                    status = "âŒ Failure"
                    status_color = "\033[91m"  # çº¢è‰²
            else:
                status = "â±ï¸  Timeout"
                status_color = "\033[93m"  # é»„è‰²
            reset_color = "\033[0m"

            # æ ¼å¼åŒ–è¾“å‡ºï¼ˆä¸è®­ç»ƒæ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
            output_parts = [f"{j:^12d}"]

            # Leaderå¥–åŠ±
            for i in range(self.n_leader):
                output_parts.append(f"{reward_leaders[i]:^12.2f}")

            # Followerå¥–åŠ±
            for k in range(self.n_follower):
                output_parts.append(f"{reward_followers[k]:^12.2f}")

            # æ­¥æ•°å’ŒçŠ¶æ€
            output_parts.append(f"{total_steps:^12d}")
            output_parts.append(f"{status_color}{status:^12}{reset_color}")

            print(" | ".join(output_parts))
        
        # è®¡ç®—æˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹çš„ç»Ÿè®¡
        success_indices = [i for i, w in enumerate(all_win) if w]
        failure_indices = [i for i, w in enumerate(all_win) if not w]
        
        # æˆåŠŸæ¡ˆä¾‹ç»Ÿè®¡
        success_stats = {}
        if len(success_indices) > 0:
            success_stats = {
                'count': len(success_indices),
                'avg_timestep': np.mean([all_ep_T[i] for i in success_indices]),
                'avg_FKR': np.mean([all_ep_F[i] for i in success_indices]),
                'avg_integral_V': np.mean([all_ep_V[i] for i in success_indices]),
                'avg_integral_U': np.mean([all_ep_U[i] for i in success_indices]),
            }
        
        # å¤±è´¥æ¡ˆä¾‹ç»Ÿè®¡
        failure_stats = {}
        if len(failure_indices) > 0:
            failure_stats = {
                'count': len(failure_indices),
                'avg_timestep': np.mean([all_ep_T[i] for i in failure_indices]),
                'avg_FKR': np.mean([all_ep_F[i] for i in failure_indices]),
            }
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*60)
        print(f"æ€»ä½“ç»Ÿè®¡:")
        print(f"  - ä»»åŠ¡å®Œæˆç‡: {win_times / test_episode:.2%}")
        print(f"  - å¹³å‡ç¼–é˜Ÿä¿æŒç‡: {average_FKR / test_episode:.4f} Â± {np.std(all_ep_F):.4f}")
        print(f"  - å¹³å‡é£è¡Œæ—¶é—´: {average_timestep / test_episode:.2f} Â± {np.std(all_ep_T):.2f}")
        print(f"  - å¹³å‡é£è¡Œè·¯ç¨‹: {average_integral_V / test_episode:.4f} Â± {np.std(all_ep_V):.4f}")
        print(f"  - å¹³å‡èƒ½é‡æŸè€—: {average_integral_U / test_episode:.4f} Â± {np.std(all_ep_U):.4f}")
        
        if success_stats:
            print(f"\nâœ… æˆåŠŸæ¡ˆä¾‹ ({success_stats['count']} æ¬¡):")
            print(f"  - å¹³å‡é£è¡Œæ—¶é—´: {success_stats['avg_timestep']:.2f}")
            print(f"  - å¹³å‡ç¼–é˜Ÿä¿æŒç‡: {success_stats['avg_FKR']:.4f}")
            print(f"  - å¹³å‡é£è¡Œè·¯ç¨‹: {success_stats['avg_integral_V']:.4f}")
            print(f"  - å¹³å‡èƒ½é‡æŸè€—: {success_stats['avg_integral_U']:.4f}")
        
        if failure_stats:
            print(f"\nâŒ å¤±è´¥æ¡ˆä¾‹ ({failure_stats['count']} æ¬¡):")
            print(f"  - å¹³å‡é£è¡Œæ—¶é—´: {failure_stats['avg_timestep']:.2f}")
            print(f"  - å¹³å‡ç¼–é˜Ÿä¿æŒç‡: {failure_stats['avg_FKR']:.4f}")
        
        print("="*60)
        
        # å…³é—­ç¯å¢ƒ
        self.env.close()
        
        # è¿”å›è¯¦ç»†ç»“æœï¼ˆæ·»åŠ æ ‡å‡†å·®å’ŒæˆåŠŸ/å¤±è´¥åˆ†æï¼‰
        results = {
            # æ€»ä½“ç»Ÿè®¡
            'win_rate': win_times / test_episode,
            'average_FKR': average_FKR / test_episode,
            'std_FKR': np.std(all_ep_F),
            'average_timestep': average_timestep / test_episode,
            'std_timestep': np.std(all_ep_T),
            'average_integral_V': average_integral_V / test_episode,
            'std_integral_V': np.std(all_ep_V),
            'average_integral_U': average_integral_U / test_episode,
            'std_integral_U': np.std(all_ep_U),
            # åŸå§‹æ•°æ®
            'all_ep_V': all_ep_V,
            'all_ep_U': all_ep_U,
            'all_ep_T': all_ep_T,
            'all_ep_F': all_ep_F,
            'all_win': all_win,
            # æˆåŠŸ/å¤±è´¥æ¡ˆä¾‹åˆ†æ
            'success_stats': success_stats,
            'failure_stats': failure_stats,
        }
        
        return results

